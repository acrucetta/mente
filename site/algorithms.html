<!DOCTYPE html><html lang='en'>
<head>
<meta charset='utf-8'>

            <meta name='viewport' content='width=device-width,initial-scale=1'>

            <link rel='stylesheet' type='text/css' href='../links/main.css'>

            <title>Mente &mdash; algorithms.htm</title>
</head>
<body>
<header>
<a href='https://andrescn.me/mente/site/about.html'><img src='../media/interface/logo.svg' alt='Mente' height='100'></a></header>
<nav>
<details open>
  <summary>Menu</summary>
  <section class="site-nav">
    <section>
      <ul class="nobull capital">
        <li><a href="about.html">About</a></li>
        <li><a href="books.html">Books</a></li>
        <li><a href="projects.html">Projects</a></li>
      </ul>
    </section>
    <section>
      <h2><a id='meta'>Meta</a></h2>
      <ul class='nobull capital'>
        <li><a href="index.html">Index</a></li>
      </ul>
  </section>
</details></nav>
<main>

<!-- Generated file, do not edit -->

<h1>algorithms.htm</h1>
<h2>Notes on Algorithms</h2>
<p><em>Algorithms: method for solving a problem.</em></p>
<p><em>Data structure: method to store information.</em></p>
<p><em>Algorithms + Data Structures = Programs</em></p>
<h2>Background: Type of Algorithms</h2>
<p>Lazy Algorithms:</p>
<ul>
<li>These algorithms delay computation or processing until its
needed.</li>
<li>They're often efficient and perform minimal work</li>
<li>They're ideal for large datasets or streams</li>
</ul>
<p>Greedy Algorithms:</p>
<ul>
<li>They make the best immediate decision without considering furhter
consequences</li>
<li>E.g., Kruskal algorithm for MST o a graph; picks the next
lowest-weight edge</li>
<li>They're often used in optimization problems; where making the
locally optimal choice leads to a globally optimal solution</li>
</ul>
<p>Eager Algorithms:</p>
<ul>
<li>They perform computation upfront; are ready with results once asked
(opposite of lazy)</li>
<li>E.g., Merge Sort sorts the entire list as soon as it's provided</li>
<li>Used when the complete results are needed immediately</li>
</ul>
<h2>Week 1: Quick Union</h2>
<p>Steps to develop a usable algorithm:</p>
<ol type="1">
<li>Model the problem</li>
<li>Find an algorithm to solve it</li>
<li>Fast enought / fits in memory?</li>
<li>If not, figure out why</li>
<li>Find a way to address the problem</li>
<li>Iterate until satisfied</li>
</ol>
<h3>Dynamic Connectivity</h3>
<p>Is there a path between two objects? Used in many applications. The
union-find is a problem of maintaining a collection of disjoint sets and
performing two operations.</p>
<p>We need to implement: find query and union command.</p>
<p>Find query: check if two objects are in the same component. Union:
replace components with their union.</p>
<p>We need to check our API design before implementing it.</p>
<p><strong>Quick Find (eager approach)</strong>:</p>
<ul>
<li>Data structure: integer array id[] of size N</li>
<li>Interpretation: two objects are connected if they have the same
ID.</li>
</ul>
<p>Cost model: numer of array accesses.</p>
<p>Find: check if p and q have the same id.</p>
<p>Union: to merge components containing p and q, change all entries
whose id equals id[p] to id[q].</p>
<p><strong>Quick Union (lazy approach):</strong></p>
<ul>
<li>Data structure: integer array id[] of size N</li>
<li>Interpretation: id[i] is parent of i</li>
<li>Root of i is id[id[id[...id[i]...]]]</li>
</ul>
<p>Find: check if p and q have the same root.</p>
<p>Union: to merge components containing p and q, set the id of p's root
to the id of q's root.</p>
<p>Quick-find: union too expensive. Trees are flat.</p>
<p>Quick-union: trees can get tall. Find too expensive (could be N array
accesses).</p>
<h3>Improvements</h3>
<p>Weighted quick-union</p>
<ul>
<li>Modify quick-union to avoid tall trees</li>
<li>Keep track of size of each tree (number of objects)</li>
<li>Balance by linking root of smaller tree to root of larger tree</li>
</ul>
<p>In sumamry, we try to avoid tall trees as we iterate through the
array.</p>
<p><img src="image-1.png" alt="weighted tree comparison" /></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuickUnion:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">id</span> <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sz <span class="op">=</span> [<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> root(<span class="va">self</span>, i):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> i <span class="op">!=</span> <span class="va">self</span>.<span class="bu">id</span>[i]:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            i <span class="op">=</span> <span class="va">self</span>.<span class="bu">id</span>[i]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> i</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> connected(<span class="va">self</span>, p, q):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.root(p) <span class="op">==</span> <span class="va">self</span>.root(q)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> union(<span class="va">self</span>, p, q):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> <span class="va">self</span>.root(p)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> <span class="va">self</span>.root(q)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">id</span>[i] <span class="op">=</span> j</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> weighted_union(<span class="va">self</span>, p, q):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> <span class="va">self</span>.root(p)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> <span class="va">self</span>.root(q)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> j:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.sz[i] <span class="op">&lt;</span> <span class="va">self</span>.sz[j]:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">id</span>[i] <span class="op">=</span> j</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sz[j] <span class="op">+=</span> <span class="va">self</span>.sz[i]</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">id</span>[j] <span class="op">=</span> i</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sz[i] <span class="op">+=</span> <span class="va">self</span>.sz[j]</span></code></pre></div>
<p><strong>Running time:</strong></p>
<ul>
<li>Find: takes time proportional to depth of p and q</li>
<li>Union: takes constant time, given roots</li>
</ul>
<p>Depth of any node x is at most lg N.</p>
<h3>Union-Find Applications</h3>
<ul>
<li>Games</li>
<li>Dynamic connectivity</li>
<li>Percolation</li>
</ul>
<p><strong>Percolation</strong></p>
<p>N by N grid sites. A system percolates iff top and botom are
connected by open sites.</p>
<p>Can be thought of as water flowing through surfaces. Or in Social
networks if we want to know whether people are connected.</p>
<p><strong>The subtext of the problem is:</strong></p>
<ul>
<li>We model the problem</li>
<li>Then we find an algorithm</li>
<li>We check whether it's fast or not</li>
<li>We address the problem</li>
<li>And iterate...</li>
</ul>
<h2>Week 2: Analysis of Algorithms</h2>
<p>The key is running time; we used to have a cranking machine; how many
cranks we need to do to compute.</p>
<p>Why analyze algorithms?</p>
<ul>
<li>Predict performance</li>
<li>Compare algorithms</li>
<li>Provide guarantees</li>
<li>Understand theoretical basis</li>
</ul>
<p>One of the most important ones is the FFT algorithm; takes only $N
log N$ steps. Another one is the N-body simulation.</p>
<p>We use the scientific method to analyze algorithms: Observe,
hypothesize, predict, verify, and validate.</p>
<p>Experiments must be <strong>reproducible</strong> and
<strong>falsifiable</strong>.</p>
<p><strong>3-Sum Example</strong></p>
<p>How many distinct integers add up to zero.</p>
<p>Brute force: do a triple for loop. (it will take n^3)</p>
<p><strong>Mathematical models of runtime</strong></p>
<p>Donald Knuth first proposed the total run-time when programs were
running for too long.</p>
<p>E.g., how many instructions as a function of input size N?</p>
<p>Turing said we should just count the most expensive operations
instead of each addition.</p>
<p>By focusing on one operation you can simplify the frequency
counts.</p>
<p><strong>Order of growth classifications</strong></p>
<p>Small set of functions: log N, N, NlogN, N^2, N^3, 2^N</p>
<p><img src="image-3.png" alt="Alt text" /></p>
<p>Based on binary search we can find a better algorithm for 3-Sum. We
can use N^2 log N instead of N^3.</p>
<p>Instructions:</p>
<ul>
<li>Sort the N integers
<ul>
<li>Insertion sort: N^2</li>
</ul></li>
<li>For each pair of integers a and b, binary search for -(a+b)
<ul>
<li>Binary search: log N</li>
</ul></li>
<li>Only count if a[i] &lt; a[j]] &lt; a[k]</li>
</ul>
<h3>Types of analysis</h3>
<p>We have best case, worst case, and average case. Lower bound on cost,
upper bound on cost, and expected cost.</p>
<p>We can have different approaches:</p>
<ol type="1">
<li>Design for worst case</li>
<li>Randomize the input</li>
</ol>
<p>The main approach is to reduce variability by focusing on the worst
case scenario. We want to find an optimal algorithm.</p>
<p>We have many notations</p>
<ul>
<li>Big Theta ($Big \theta$): asymptotic order of growth</li>
<li>Big Oh: to develop upper bounds</li>
<li>Big Omega: to develop lower bounds</li>
</ul>
<p>Example:</p>
<ul>
<li>3 Sum
<ul>
<li>Improved algorithm gives us O($N^2logN$)</li>
<li>Lower bound (proof that no algorithm can do better):
$\omega(N)$</li>
</ul></li>
</ul>
<p>The approach:</p>
<ul>
<li>Develop algorithm</li>
<li>Prove a lower bound</li>
</ul>
<p>We can also have tilde notation. It's used to provide an approximate
model.</p>
<h3>Memory</h3>
<p>Typical memory usage for primitive types:</p>
<ul>
<li>Boolean (1); Char (2); Double (8); Int (8)</li>
<li>Int[][] ~4MN; int[] ~4N+24;</li>
</ul>
<p>Typical memory usage for objects in Java:</p>
<ul>
<li>Object overhead 16 bytes</li>
<li>Ref 8 bytes</li>
<li>Padding multiple of 8 bytes</li>
</ul>
<h2>Week 2: Stacks and Queues</h2>
<h3>Stacks and queues</h3>
<ul>
<li>They're fundamental data types</li>
<li>Stack has push and pop (LIFO)</li>
<li>Queue has enqueue and dequeue (FIFO)</li>
</ul>
<h3>Stacks</h3>
<ul>
<li>push(); pop(); isEmpty()</li>
<li>We build the stack with a LinkedList
<ul>
<li>push - insert node to the beginning</li>
<li>pop - remove node from the beginning</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb2"><pre
class="sourceCode java"><code class="sourceCode java"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pop</span><span class="op">()</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">String</span> item <span class="op">=</span> first<span class="op">.</span><span class="fu">item</span><span class="op">;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>first <span class="op">=</span> first<span class="op">.</span><span class="fu">next</span><span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">push</span><span class="op">()</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">Node</span> oldfirst <span class="op">=</span> first<span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>first <span class="op">=</span> <span class="kw">new</span> <span class="bu">Node</span><span class="op">();</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>first<span class="op">.</span><span class="fu">item</span> <span class="op">=</span> <span class="st">&quot;not&quot;</span><span class="op">;</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>first<span class="op">.</span><span class="fu">next</span><span class="op">=</span> oldfirst</span></code></pre></div>
<ul>
<li>Every op takes constant time in the worst case.</li>
<li>A tack with N items uses ~40N bytes.</li>
<li>Every object in Java has 16 bytes of overhead.</li>
</ul>
<p><strong>Alternative Implementation</strong></p>
<ul>
<li>Use array s[] to store N items on stack</li>
<li>push(): add new item at S[N]</li>
<li>pop(): remove item from S[N-1]</li>
<li>Cons: need to define the capacity ahead of time</li>
</ul>
<p>We have to worry about loitering in Java. To avoid that we need to
set the removed item to Null so it can be reclaimed by the garbage
collector.</p>
<p><strong>Resizing arrays implementations</strong></p>
<p>Q: How can we grow the array? A: If the array is full, create a new
array of twice the size and copy the items. ~3N.</p>
<p>Q: How to shrink array? A: Wait until the array is one-quarter full.
Invariant; if the array is always between 25% and 100%</p>
<p>The worst case for push and pop will be N.</p>
<p><strong>Memory:</strong> It uses between 8N and 32N.</p>
<h3>Tradeoffs</h3>
<ul>
<li>Linked list:
<ul>
<li>Every op takes constant time in worst case</li>
<li>We need to use extra time and space to deal with the links</li>
</ul></li>
<li>Stack
<ul>
<li>Every op takes constant amortized time</li>
<li>Less wasted space</li>
</ul></li>
</ul>
<h3>Queues</h3>
<p><strong>LinkedList</strong></p>
<pre><code>enqueue()

String item = first.item;
first = first.next;
return item;

dequeue()

Node oldlast = last;
Node last = new Node();
last.item =&quot;not&quot;;
last.next = null;

oldlast.next = last;</code></pre>
<h3>Generics</h3>
<p>We have implemented stacks of strings, but what about URLs, Ints,
Vans...</p>
<p>We use the generic type name in Java <code>&lt;Item&gt;</code></p>
<p>Java doesn't allow generic array creation. We need to create an array
of Objects and pass it to a list of items.</p>
<pre><code>s = (Item[]) new Object([Capacity])</code></pre>
<p>Q: What to do about primitive types?</p>
<p>Wrapper type: each primitive type has a wrapper object type. Integer
is a wrapper type for int.</p>
<h3>Iteration</h3>
<p>Design challenge: How can we support iteration over stack items by
items.</p>
<p>An Iterable is a method that returns an Iterator. It has methods
hasNext() and next()</p>
<p>Why make data structures Iterable? It can helps us write elegant
code. Allows us to use <code>for each statements</code></p>
<h3>Week 2: Self Questions</h3>
<ul>
<li>Why does the stack have contant push and pop time?
<ul>
<li>Because we just need to return the first item or last item in the
structure without iterating over its values</li>
</ul></li>
<li>What are advantages of a LinkedList over a stack?
<ul>
<li>Arrays provide immediate access to any item; but we need to know the
size on initialization</li>
<li>Linked List use space proportional to size but we need a reference
to access an item</li>
</ul></li>
<li>When do we use the LinkedList, when do we use the stack
<ul>
<li>Depending on the access patterns and memory consumption; stacks are
easy for push/pop but to find items it takes a bit more time. We do have
resizing issues.</li>
</ul></li>
<li>What's the time complexity of each one?</li>
<li>How does Java deal with Generics and Iterators?</li>
<li>Why is it better to use Iterators? What does that allows us to
do?</li>
<li>What is the origin of LinkedLists?
<ul>
<li>They were initially used by LISP in 1950s as the primary structure
for all programs and data. IT presented challenges because they're hard
to debug.</li>
</ul></li>
<li>Why can't we remove an item a O(n) complexity from a Stack?
<ul>
<li>Because we still have to shift all the other positions next to the
item removed. This would cost a linear amount of time.</li>
</ul></li>
</ul>
<h2>Week 2: Sorting</h2>
<p><strong>Q:</strong> How does Sort knows how to sort Doubles, Strings,
and files without any information?</p>
<ul>
<li>We use callbacks: the sort() function calls the object's compareTo
method.</li>
<li>Commonly implemented in other programming languages.</li>
</ul>
<p>The sort implementation takes a Comparable[] object. Then call the
<code>.compareTo()</code> code.</p>
<p>Yes, you're right. Let's break down the definitions for each of these
properties in the context of a binary relation ( $\leq$ ) on a set ( S
):</p>
<ol type="1">
<li><p><strong>Antisymmetry</strong>:</p>
<ul>
<li>This means that two distinct elements cannot both be "less than or
equal to" each other. If they are, they must actually be the same
element.</li>
</ul></li>
<li><p><strong>Transitivity</strong>:</p>
<ul>
<li>This captures the intuitive idea that if ( a ) is "less than or
equal to" ( b ) and ( b ) is "less than or equal to" ( c ), then ( a )
should also be "less than or equal to" ( c ).</li>
</ul></li>
<li><p><strong>Totality (or Completeness)</strong>:</p>
<ul>
<li>This means that given any two elements in the set, one must be "less
than or equal to" the other or vice-versa. This ensures that there are
no two elements which can't be compared in the order.</li>
</ul></li>
</ol>
<p>Ex standard order for natural and real numbers, alphabetical orders,
chronological orders.</p>
<p><strong>Comparable API</strong></p>
<ul>
<li>Less than return -1</li>
<li>Equal return 0</li>
<li>More than return +1</li>
</ul>
<h3>Selection Sort</h3>
<p>In iteration I, find index min of smallest remaining entry; swap a[I]
and a[min]</p>
<p>Algorithm: scans from left to right.</p>
<p>Selection sort uses $~N^2/2$ compares. It takes quadratic time, even
if the input is sorted.</p>
<p><img
src="https://www.programiz.com/sites/tutorial2program/files/Selection-sort-0.png" /></p>
<h3>Insertion Sort</h3>
<p>In iteration I we swap a[i] with each larger entry to its left.</p>
<p>To sort a randomly ordered arra with distinct keys, it uses $~1/4
N^2$ compares and $~1/4N^2$ exchanges.</p>
<p>If the array is in reverse order, it has to go all the way back to
the beginning; if it's in ascending order, it takes 0 exchanges.</p>
<p>For partially sorted arrays, it takes linear time. An array is
partially sorted if the number of inversions is less than some constant
cN.</p>
<p><img
src="https://media.geeksforgeeks.org/wp-content/uploads/insertionsort.png"
alt="insertion sort" /></p>
<h3>Shellsort</h3>
<p>Move entries more than one position at a time by h-sorting the array.
An h-sorted array is h interleaved sorted subsequences. (1959 sorting
method)</p>
<p>H-sort is just insertion sort with stride length h. Why do this?</p>
<ul>
<li>Big increments -&gt; small subarray</li>
<li>Small increments -&gt; nearly in order</li>
</ul>
<p>Which increment sequence to use?</p>
<ul>
<li>Powers of two? Powers of two minus one?</li>
<li>3x+1? Easy to compute</li>
<li>Sedgewick: 1,5,19,41...</li>
</ul>
<p>The worst case number of compares used by shell sort is
$O(N^(3/2))$</p>
<p>Shell sort is a simple idea leading to substantial performance gains.
It involves very little code, used often in hardware (embedded
systems)</p>
<p><img
src="https://static.javatpoint.com/ds/images/shell-sort-algorithm2.png"
alt="shell sort" /></p>
<h3>Shuffle sort</h3>
<ul>
<li>Generate a random real number for each array entry</li>
<li>Sort using these random numbers as the key; uniformly random
permutation</li>
</ul>
<p>Knuth sort: in iteration I, pick integer r between 0 and I uniformly
at random. Then we swap a[I] and a[r].</p>
<h3>Convex Hull</h3>
<p>The convex hull of a set of N points is the smallest perimeter fence
enclosing the points.</p>
<p><img
src="https://miro.medium.com/v2/resize:fit:677/1*F4IUmOJbbLMJiTgHxpoc7Q.png" /></p>
<p>E.g.,</p>
<ul>
<li>find shortest path in the plane from s to t avoiding a polygonal
obstacle.</li>
<li>find the farthest path between two points</li>
</ul>
<p>Graham scan demo</p>
<ul>
<li>Choose point p with smallest y-coordinate</li>
<li>Sort points by polar angle with p</li>
<li>Consider points in oder,, discard unless it creates a ccw turn</li>
</ul>
<p><strong>Week 3: Self-Questions</strong></p>
<ul>
<li>When should I use which sort?
<ul>
<li>Insertion Sort</li>
<li>Select Sort</li>
<li>Merge Sort</li>
<li>Shell Sort</li>
<li>Shuffling</li>
</ul></li>
<li>Why are some sorts better than others in specific occasions?</li>
<li>What are the most widely used sorts?</li>
</ul>
<h2>Week 3: Merge Sort</h2>
<ul>
<li>Divide array into two halves</li>
<li>Recursively sort each array</li>
<li>Merge the two halves</li>
</ul>
<p>Copy to an auxiliary array. ** <img
src="https://www.programiz.com/sites/tutorial2program/files/merge-sort-example_0.png"
alt="mergesort" /></p>
<p>Mergesort uses at most NlgN compares and 6NlgN array accesses to sort
any array of size N.</p>
<p>Mergesort uses extra space proportions to N.</p>
<p>Practical improvements:</p>
<ul>
<li>Use insertion sort for small subarrays; merge sort has too much
overhead for tiny arrays</li>
<li>The cut-off for mergesort is 7 items</li>
<li>Stop if already sorted; is the biggest item in first half &lt;
smallest item in second half</li>
</ul>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># split in half</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> n <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># recursive sorts</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>sort a[<span class="fl">1.</span>.m]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>sort a[m<span class="op">+</span><span class="fl">1.</span>.n]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># merge sorted sub-arrays using temp array</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> copy of a[<span class="fl">1.</span>.m]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span>, j <span class="op">=</span> m<span class="op">+</span><span class="dv">1</span>, k <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> i <span class="op">&lt;=</span> m <span class="kw">and</span> j <span class="op">&lt;=</span> n,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    a[k<span class="op">++</span>] <span class="op">=</span> (a[j] <span class="op">&lt;</span> b[i]) ? a[j<span class="op">++</span>] : b[i<span class="op">++</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    → invariant: a[<span class="fl">1.</span>.k] <span class="kw">in</span> final position</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> i <span class="op">&lt;=</span> m,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    a[k<span class="op">++</span>] <span class="op">=</span> b[i<span class="op">++</span>]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    → invariant: a[<span class="fl">1.</span>.k] <span class="kw">in</span> final position</span></code></pre></div>
<h3>Bottom-up Merge Sort (no recursion)</h3>
<p>Basic plan</p>
<ul>
<li>Pass through array merging subarrays of size 1</li>
<li>Repeat for subarrays of size 2,4,8,16...</li>
<li>Uses space proportional to the array</li>
</ul>
<h3>Sorting Complexity</h3>
<p>Computational complexity: framework to study efficiency of algorithms
to solve problem X.</p>
<ul>
<li>Model of computation: allowable ops.</li>
<li>Cost model: operation counts.</li>
<li>Upper bound: cost guarantee provided by some algorithm</li>
<li>Lower bound: Proven limit on cost for all algorithms of X</li>
<li>Optimal algorithm: best possible cost guarantee (lower bound - upper
bound)</li>
</ul>
<p><strong>E.g. Sorting</strong></p>
<ul>
<li>Model of computation: decision tree</li>
<li>Cost model: # of compares</li>
<li>Upper bound: ~N log N</li>
<li>Lower bound: ~N log N</li>
<li>Optimal algorithm: mergesort
<ul>
<li>Optimal with respect to # of compares</li>
<li>Not optimal with respect to usage</li>
<li>We use theory as guide</li>
</ul></li>
</ul>
<h3>Comparators</h3>
<p>We also have a comparator interface to sort using an alternate
order.</p>
<p><code>int compare(Key v, Key w)</code></p>
<p>E.g.,</p>
<ul>
<li>Compare students by name, by section, etc...</li>
</ul>
<div class="sourceCode" id="cb6"><pre
class="sourceCode java"><code class="sourceCode java"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">Arrays</span><span class="op">.</span><span class="fu">sort</span><span class="op">(</span>a<span class="op">,</span> Student<span class="op">.</span><span class="fu">BY_NAME</span><span class="op">)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">Arrays</span><span class="op">.</span><span class="fu">sort</span><span class="op">(</span>points<span class="op">,</span> p<span class="op">.</span><span class="fu">POLAR_ORDER</span><span class="op">)</span></span></code></pre></div>
<h3>Stability</h3>
<p>A typical application: first sort by name, then by section.</p>
<p>A stable sort is one that presents the same order of items with a
given key.</p>
<p>Insertion sort and merge sort are stable (but not selection sort or
shell sort)</p>
<p>We can move items past items that are equal.</p>
<h3>Self Questions</h3>
<ul>
<li>When do we care about stability for sorting</li>
<li>Why are some algorithms stable and others aren't?</li>
</ul>
<h3>Week 3: Quick-sort</h3>
<p>Basic Plan:</p>
<ul>
<li>Shuffle the array</li>
<li>Partition so that, for some k
<ul>
<li>entry a[j] is in place</li>
<li>no larger entry to the left of j</li>
<li>no smaller entry to the right of j</li>
</ul></li>
<li>Sort each piece recursively</li>
</ul>
<p>Summary:</p>
<ul>
<li>We keep swapping 2 pointers as long as the left is less than the
right</li>
<li>Once they cross we stop and swap the lo with J</li>
</ul>
<p>Implementation details:</p>
<ul>
<li>Partitioning is in-place: using an extra array can make partitioning
easier</li>
<li>Terminating the loop: tricky to check if pointers cross</li>
<li>Shuffling is needed for the performance guarantee</li>
</ul>
<p>Performance:</p>
<ul>
<li>Number of compares 1.39 N Log N (39% more than merge sort)</li>
<li>Worst case is quadratic but not likely</li>
</ul>
<p>Properties:</p>
<ul>
<li>Quick-sort is an in-place sorting algorithm</li>
<li>Quick sort is not stable</li>
<li>We can improve it with median-of-3 and cutoff to insertion sort</li>
</ul>
<h3>Week 3: Selection Sort</h3>
<p><strong>Goal</strong>: Given an array of N items, find the Kth
largest. i.e., return the top K.</p>
<p>Quick Select:</p>
<ul>
<li>Partition array so that: entry a[j] is in place</li>
<li>No larger entry to the left of j</li>
<li>No smaller entry to the right of j</li>
<li>Repeat in one subarray, depending on j; finished when j equals
k.</li>
</ul>
<p>3-Way Quick sort (Dijkstra Implementation)</p>
<ul>
<li>We have 3 pointers; move the ones less than the value to the
left</li>
<li>Move the ones higher than the value to the right</li>
<li>We increment the pointer if the value is equal to the current
value</li>
<li>We end up with all values even the duplicates sorted</li>
</ul>
<h3>Week 3: Applications</h3>
<ul>
<li>Sort a list of names</li>
<li>Find the median</li>
<li>Data compression</li>
<li>Computer graphics</li>
<li>Load balancing on a parallel computer</li>
</ul>
<p><strong>Each sort type will have different attributes:</strong></p>
<ul>
<li>Stable</li>
<li>Parallel</li>
<li>Deterministic</li>
<li>Keys all distinct</li>
<li>Multiple key types</li>
<li>Linked lists or arrays</li>
<li>Large or small items</li>
<li>Is the array randomly ordered</li>
<li>Do we need a guaranteed performance</li>
</ul>
<p>System sorts cannot possibly cover all possible algorithms.</p>
<h3>Week 3: Self Questions</h3>
<ul>
<li>Why does Arrays.sort() Arrays.sort() in Java use mergesort instead
of quicksort when sorting reference types?
<ul>
<li>The Java API for Arrays.sort() for reference types requires that it
is stable and guarantees nlogn performance. Neither of these are
properties of standard quicksort.</li>
</ul></li>
</ul>
<h2>Week 4</h2>
<h3>Priority Queues API</h3>
<p>Collections:</p>
<ul>
<li>Stack</li>
<li>Queue</li>
<li>Randomized queue</li>
<li>Priority queue - remove largest or smallest item</li>
</ul>
<p>Applications:</p>
<ul>
<li>Statistics</li>
<li>Artificial Intelligence</li>
<li>Number theory</li>
<li>Graph searching</li>
<li>Operating systems - load balancing</li>
</ul>
<p>Implementation:</p>
<ul>
<li>It allows us to find the largest M items in a steam of N items in
NlogM (as compared with other data structures that have NM)</li>
</ul>
<p>The binary heap comes from a complete binary tree. Perfectly balanced
except for the bottom level.</p>
<p>Heap-ordered binary tree</p>
<ul>
<li>Keys in nodes</li>
<li>Parent's key no smaller than children's keys</li>
</ul>
<p><img
src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20221220165711/MinHeapAndMaxHeap1.png"
alt="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20221220165711/MinHeapAndMaxHeap1.png" /></p>
<p>If we insert a key larger than its parents we exchange them until the
order is restored.</p>
<p>To insert into a heap, we add a node at the end then swim it up with
at most 1 + lg N compares. AKA Peter principle; promoted to level of
incompetence.</p>
<p>If the parent's key becomes smaller than one or both, we figure out
which children is higher then exchange them until the order is restored
(recursively). AKA better subordinate promoted (power struggle)</p>
<p>To delete the maximum, we exchange root with node at end, then sink
it down. At most 2 Log N compares.</p>
<p>Considerations:</p>
<ul>
<li>Client does not change keys while they're on the PQ</li>
<li>Best practice: use immutable keys</li>
</ul>
<p>Java Note on Immutability: when we use <em>final</em> those values
can't change once created.</p>
<p>Immutable: String, Integer, Double, Color, Vector, Transaction...</p>
<p>Why use?</p>
<ul>
<li>Simplifies debugging</li>
<li>Safer</li>
<li>Simplifies concurrent programming</li>
<li>Safe to use as key</li>
</ul>
<p><em>We should make classes immutable as much as possible - Creator of
Java</em></p>
<h3>Week 3: Heapsort</h3>
<ul>
<li>We start with an unordered list of numbers; we transform it into a
max heap ("heapify")</li>
<li>Then we replace the item at the root with the last item. Repeating
until the size of the heap is greater than 1</li>
</ul>
<p><strong>Complexity</strong></p>
<ul>
<li>Best Case: O(n log n)</li>
<li>Average Case: O(n log n)</li>
<li>Worst Case: O(n log n)</li>
</ul>
<p>Significance: In-place sorting algorithm with NlogN worst-case.</p>
<ul>
<li>Mergesort: no, linear extra space</li>
<li>Quicksort, no, quadratic time in worst case</li>
<li>Heapsort: yes</li>
</ul>
<h2>Week 4: Symbol Tables</h2>
<ul>
<li>Insert a value with a specified key</li>
<li>Given a key, search for the corresponding value</li>
</ul>
<p><strong>DNS lookup</strong></p>
<ul>
<li>Insert URL with specified IP address</li>
<li>Given URL, find corresponding IP address</li>
</ul>
<p>API Examples</p>
<ul>
<li>put</li>
<li>get</li>
<li>delete</li>
<li>contains</li>
<li>isEmpty</li>
<li>size</li>
<li>keys</li>
</ul>
<p>We associate one value with each key.</p>
<p>Conventions:</p>
<ul>
<li>Values are not null</li>
<li>Method get() returns null if key not present</li>
<li>Method put() overwrites old value with new value</li>
</ul>
<p>Value type: any generic type.</p>
<ul>
<li>Assume keys are Comparable: we use compareTo()</li>
<li>Assume keys are any generic type; use equals() to est equality</li>
</ul>
<p>Best practices - use immutable types for symbol table keys.</p>
<ul>
<li>Immutables in Java: String ,Integer, Double, File...</li>
<li>Mutable in Java: StringBuilder, java.net, URL, arrays</li>
</ul>
<p>All Java classes inherit a method equals()</p>
<ul>
<li>Reflexive</li>
<li>Symmetric</li>
<li>Transitive</li>
<li>Non-null</li>
</ul>
<h3>Symbol Table Implementations</h3>
<p><strong>Sequential Search (Unordered LinkedList)</strong></p>
<ul>
<li>Performance
<ul>
<li>Search - N</li>
<li>Insert - N</li>
<li>Search Hit - N/2</li>
<li>Inser - N</li>
</ul></li>
<li>Key Interface
<ul>
<li>equals()</li>
</ul></li>
</ul>
<p>We use a LinkedList in this implementation.</p>
<p><strong>Binary search in an ordered array</strong></p>
<ul>
<li>Data structure: maintain an ordered array of key-value pairs</li>
<li>We use a rank helper function to find how many keys &lt; K</li>
<li>Performance
<ul>
<li>Worst
<ul>
<li>Search - log N</li>
<li>Insert N</li>
</ul></li>
<li>Avg Case
<ul>
<li>Search Hit - log N</li>
<li>Inser - N/2</li>
</ul></li>
</ul></li>
<li>Key Interface:
<ul>
<li>compareTo()</li>
</ul></li>
</ul>
<h3>Ordered Operations</h3>
<ul>
<li>We want to be able to do min(); get(); floor(); select(7); ceiling()
etc... on a symbol table</li>
<li>These ops come natural in a binary search operation</li>
</ul>
<p>We can add more operations to a Symbol Table such as:</p>
<ul>
<li>deleteMin()</li>
<li>deleteMax()</li>
<li>select()</li>
<li>rank()</li>
<li>min() / max()</li>
<li>ordered iteration</li>
</ul>
<p>We normally argue against wide interfaces.</p>
<h3>Binary Search Trees</h3>
<p>A BST is a binary tree in symmetric order.</p>
<p>It is either:</p>
<ul>
<li>Empty</li>
<li>Two disjoint binary trees (left and right)</li>
</ul>
<p>Each node has a key and every node's key is:</p>
<ul>
<li>Larger than all the keys in its left subtree</li>
<li>Smaller than all the keys in its right subtree</li>
</ul>
<pre><code>
private class Node
    /

</main>
<footer id='end_footer'><hr />
<span style='float:right'>Edited on 2024-01-04</span><b>Mente</b> © 2023 — 
</footer>
</body></html>
