<!DOCTYPE html><html lang='en'>
<head>
<meta charset='utf-8'>

            <meta name='viewport' content='width=device-width,initial-scale=1'>

            <link rel='stylesheet' type='text/css' href='../links/main.css'>

            <title>Mente &mdash; data_engineering.htm</title>
</head>
<body>
<header>
<a href='https://andrescn.me/mente/site/about.html'><img src='../media/interface/logo.svg' alt='Mente' height='100'></a></header>
<nav>
<details open>
  <summary>Menu</summary>
  <section class="site-nav">
    <section>
      <ul class="nobull capital">
        <li><a href="about.html">About</a></li>
        <li><a href="books.html">Books</a></li>
        <li><a href="projects.html">Projects</a></li>
      </ul>
    </section>
    <section>
      <h2><a id='meta'>Meta</a></h2>
      <ul class='nobull capital'>
        <li><a href="index.html">Index</a></li>
      </ul>
  </section>
</details></nav>
<main>

<!-- Generated file, do not edit -->

<h1>data_engineering.htm</h1>
<p><a href='books.html' class='local'>books</a></p>
<h2>Chapter 1: Overview of Data Engineering</h2>
<p><img src="https://www.oreilly.com/api/v2/epubs/9781098108298/files/assets/fode_0201.png" alt="lifecycle" /></p>
<h3>Data Engineering Teams</h3>
<p>The data engineer is a hub between data producers, such as software engineers, data architects, and DevOps or site-reliability engineers (SREs), and data consumers, such as data analysts, data scientists, and ML engineers. In addition, data engineers will interact with those in operational roles, such as DevOps engineers.</p>
<h3>ML Engineers vs. Data Engineer</h3>
<p>The ML engineer overlaps with DE, but it develops more advanced ML techniques, train models, and designs and maintains infrastructure running ML processes. It emphasizes more MLOps and other mature practices such as DevOps.</p>
<h2>Chapter 2: Main components of Data Engineering</h2>
<h3>Data Generation</h3>
<ul>
<li>Data is coming from a variety of sources now</li>
<li>It's hard to keep in the same schema. Schema evolution is now considered part of the Agile approach.</li>
</ul>
<h3>Storage</h3>
<ul>
<li>Cloud architectures often have several storage solutions</li>
<li>Hot data is data commonly retrieved many times per day. It should be geared for fast retrieval</li>
<li>Cold data is seldom queried and mostly for archival purposes.</li>
</ul>
<h3>Ingestion</h3>
<ul>
<li>A common bottleneck of the lifecycle; source systems are outside of your direct control and might have data of poor quality</li>
<li>The separation of storage and compute makes streaming more popular</li>
<li>Can my downstream services handle the volume of data I'm sending them?</li>
<li>Batch is still widely used because of legacy systems, streaming can be useful but it creates increasing amounts of complexity</li>
</ul>
<p><strong>Key Questions:</strong></p>
<ul>
<li>What are the use cases for the data?</li>
<li>What is the destination?</li>
<li>In what volume will it arrive?</li>
<li>What format is the data in?</li>
</ul>
<h3>Transformation</h3>
<ul>
<li>Change data into correct types, put records into standard formats, remove bad ones</li>
<li>Large-scale aggregation for reporting or featurizing data for ML processes</li>
</ul>
<h3>Serving</h3>
<ul>
<li>Data is served for analytics, ML, and reverse ETL</li>
<li>Before investing into ML, companies need to build a solid data foundation</li>
<li>This means setting up the best systems and architecttures across the data engineering and ML lifecycle.</li>
<li>Analytics then ML</li>
<li>Reverse ETLs feed bvack the data into source systems. It's useful for businesses relying on SaaS.</li>
</ul>
<h3>Data Ops</h3>
<ul>
<li>DataOps automation has a similar framework and workflow to DevOps, consisting of change management (environment, code, and data version control), continuous integration/continuous deployment (CI/CD), and configuration as code.</li>
<li>DatOps consist of automation, observability and monitoring, and incident response</li>
</ul>
<h4>Observability and Monitoring</h4>
<ul>
<li>Data is a silent killer; bad data can linger in reports for a long time</li>
<li>You need to implement observing, monitoring, logging, alerting, and tracing </li>
<li>DODD focused on making data observability a first-class consideration in the data engineering lifecycle</li>
</ul>
<h3>Data Architecture</h3>
<h4>Types of data architectures</h4>
<ul>
<li>Data warehouse: structured, includes the compute
<ul>
<li>A subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of mgmt decisions</li>
<li>Separates OLAP from OLTP</li>
<li>Centralized and organizes data</li>
<li>e.g., Snowflake, Amazon Redshift</li>
</ul>
</li>
<li>Data lake: unstructured, data can be in any format
<ul>
<li>Common during big data era</li>
<li>Data can be queried with MapReduce, Spark, Presto, etc...</li>
</ul>
</li>
<li>Data lakehouse: combines benefits from both, introduced by Databricks
<ul>
<li>Includes control, data management and data structures</li>
<li>Still houses data in an object storage and supports a variety of query and transformation engines</li>
</ul>
</li>
</ul>
<h4>Modern Data Stack</h4>
<ul>
<li>Instead of using monolithic toolsets, use cloud-based, plug and play, off-the-shelf components</li>
<li>Includes data pipelines, storage, monitoring, etc...</li>
</ul>
<h2>Chapter 5: Data Generation</h2>
<h3>Data Logs</h3>
<ul>
<li>Insert only: always retain the records, add a timestamp</li>
</ul>
<h3>Messages and Streams</h3>
<ul>
<li>Message queues and streaming platforms are often used interchangeably</li>
<li>A message is raw data communicated across two or more systems
<ul>
<li>A message is normally sent through a message queue from a publisher to a consumer</li>
<li>Once the message is delivered, it is removed from the queue</li>
</ul>
</li>
<li>A stream is an append only log of event records
<ul>
<li>You use streams when you care about what happened over many events</li>
</ul>
</li>
</ul>
<h3>Types of time</h3>
<ul>
<li>Event time; event is generated</li>
<li>Ingestion time; event is ingested</li>
<li>Process time; event is processed</li>
</ul>
<h3>Ways of ingesting data</h3>
<ul>
<li>APIs
<ul>
<li>REST</li>
<li>GraphQL</li>
<li>WebHooks
<ul>
<li>Event based data transmission pattern</li>
<li>Data source can be an application backend, a web page, or a mobile app</li>
</ul>
</li>
<li>RPC and gRPC: remote procedure call library; bidirectional exchange of data over HTTP/2
<ul>
<li>Imposes more technical standards than REST</li>
</ul>
</li>
</ul>
</li>
<li>Message queues
<ul>
<li>Critical for decoupled microservices and event-driven architectures</li>
<li>Need to keep in mind the frequency of delivery, message ordering and scalability</li>
<li>Some issues with queues can be the order of the messages</li>
<li>Ideally, should be idempotent; outcome is the same after processing it multiple times</li>
</ul>
</li>
<li>Event streaming platform
<ul>
<li>Produces data in an ordered log of records</li>
<li>Includes
<ul>
<li>Topics - collection of related events</li>
<li>Stream partitions</li>
</ul>
</li>
<li>Tend to be more fault tolerant because they're distributed</li>
</ul>
</li>
</ul>
<h3>Ingestion undercurrents</h3>
<ul>
<li>Security: how is the data secured and encrypted; do we have a VPN, or an SSO?</li>
<li>Data management: who manages the data, what's the quality, what if the schema changes, is the data HIPAA?</li>
<li>DataOps: how will we know when there's an issue, how are we monitoring, is the schema conformant, what do we do if something bad happens</li>
<li>Architecture: what if the system fails, what do we do if we lose data, how available are the sources, who's in charge of them</li>
<li>Orchestration: how often do we recieve the data, can we integrate with the upstream application team</li>
<li>SWE: Can the code access with the right credentials, how do we authenticate, how do we access (API, REST), can we parallelize the work, how do we deploy code changes?</li>
</ul>
<h2>Chapter 7: Storage</h2>
<p>The raw ingredients are: disk drives, memory, networking and CUPU, serialization, compression and caching</p>
<h3>Raw Ingredients</h3>
<ul>
<li>Magnetic Disk Drives
<ul>
<li>They can be slow but are cheap</li>
<li>They are constrained by disk transfer speed and rotational latency</li>
<li>Storing info in parallel can make them faster</li>
<li>The max speed is 200-300 MB/s (20 hours for 30 TB)</li>
<li>Used widely because of how cheap they are</li>
<li>If we use multiple, the bottleneck becomes network and CPU</li>
</ul>
</li>
<li>Solid State Drives
<ul>
<li>Stores data in flash memory cells</li>
<li>They're really fast (0.1 ms)</li>
<li>Used for OLTP Systems; allow Postgres</li>
<li>Cost 20-30 cents per GB (10x magnetic drives)</li>
<li>SSD can be used in some OLAP for caching</li>
</ul>
</li>
<li>RAM
<ul>
<li>Attached to CPU; stores code</li>
<li>It's volatile, if it fails it gets reset</li>
<li>1000 times faster than SSD</li>
<li>$10/GB</li>
<li>Stores data in capacitors, needs to be refreshed over time</li>
</ul>
</li>
<li>Networking and CPU
<ul>
<li>Availability zones impact the storage access</li>
<li>Trade off in spreading data vs. keeping it under on zone</li>
</ul>
</li>
</ul>


</main>
<footer><hr />
<span style='float:right'>Edited on 2023-07-19</span><b>Mente</b> © 2023 — 
</footer>
</body></html>
