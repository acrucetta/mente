<!DOCTYPE html><html lang='en'>
<head>
<meta charset='utf-8'>

            <meta name='viewport' content='width=device-width,initial-scale=1'>

            <link rel='stylesheet' type='text/css' href='../links/main.css'>

            <title>Mente &mdash; data_engineering.htm</title>
</head>
<body>
<header>
<a href='https://andrescn.me/mente/site/about.html'><img src='../media/interface/logo.svg' alt='Mente' height='100'></a></header>
<nav>
<details open>
  <summary>Menu</summary>
  <section class="site-nav">
    <section>
      <ul class="nobull capital">
        <li><a href="about.html">About</a></li>
        <li><a href="books.html">Books</a></li>
        <li><a href="projects.html">Projects</a></li>
      </ul>
    </section>
    <section>
      <h2><a id='meta'>Meta</a></h2>
      <ul class='nobull capital'>
        <li><a href="index.html">Index</a></li>
      </ul>
  </section>
</details></nav>
<main>

<!-- Generated file, do not edit -->

<h1>data_engineering.htm</h1>
<h2 id="overview-of-data-engineering">Overview of Data Engineering</h2>
<figure>
<img
src="https://www.oreilly.com/api/v2/epubs/9781098108298/files/assets/fode_0201.png"
alt="lifecycle" />
<figcaption aria-hidden="true">lifecycle</figcaption>
</figure>
<h3 id="data-engineering-teams">Data Engineering Teams</h3>
<p>The data engineer is a hub between data producers, such as software
engineers, data architects, and DevOps or site-reliability engineers
(SREs), and data consumers, such as data analysts, data scientists, and
ML engineers. In addition, data engineers will interact with those in
operational roles, such as DevOps engineers.</p>
<h3 id="ml-engineers-vs.-data-engineer">ML Engineers vs. Data
Engineer</h3>
<p>The ML engineer overlaps with DE, but it develops more advanced ML
techniques, train models, and designs and maintains infrastructure
running ML processes. It emphasizes more MLOps and other mature
practices such as DevOps.</p>
<h2 id="main-components-of-data-engineering">Main components of Data
Engineering</h2>
<h3 id="data-generation">Data Generation</h3>
<ul>
<li>Data is coming from a variety of sources now</li>
<li>It’s hard to keep in the same schema. Schema evolution is now
considered part of the Agile approach.</li>
</ul>
<h3 id="storage">Storage</h3>
<ul>
<li>Cloud architectures often have several storage solutions</li>
<li>Hot data is data commonly retrieved many times per day. It should be
geared for fast retrieval</li>
<li>Cold data is seldom queried and mostly for archival purposes.</li>
</ul>
<h3 id="ingestion">Ingestion</h3>
<ul>
<li>A common bottleneck of the lifecycle; source systems are outside of
your direct control and might have data of poor quality</li>
<li>The separation of storage and compute makes streaming more
popular</li>
<li>Can my downstream services handle the volume of data I’m sending
them?</li>
<li>Batch is still widely used because of legacy systems, streaming can
be useful but it creates increasing amounts of complexity</li>
</ul>
<p><strong>Key Questions:</strong> - What are the use cases for the
data? - What is the destination? - In what volume will it arrive? - What
format is the data in?</p>
<h3 id="transformation">Transformation</h3>
<ul>
<li>Change data into correct types, put records into standard formats,
remove bad ones</li>
<li>Large-scale aggregation for reporting or featurizing data for ML
processes</li>
</ul>
<h3 id="serving">Serving</h3>
<ul>
<li>Data is served for analytics, ML, and reverse ETL</li>
<li>Before investing into ML, companies need to build a solid data
foundation</li>
<li>This means setting up the best systems and architecttures across the
data engineering and ML lifecycle.</li>
<li>Analytics then ML</li>
<li>Reverse ETLs feed bvack the data into source systems. It’s useful
for businesses relying on SaaS.</li>
</ul>
<h3 id="data-ops">Data Ops</h3>
<ul>
<li>DataOps automation has a similar framework and workflow to DevOps,
consisting of change management (environment, code, and data version
control), continuous integration/continuous deployment (CI/CD), and
configuration as code.</li>
<li>DatOps consist of automation, observability and monitoring, and
incident response</li>
</ul>
<h3 id="observability-and-monitoring">Observability and Monitoring</h3>
<ul>
<li>Data is a silent killer; bad data can linger in reports for a long
time</li>
<li>You need to implement observing, monitoring, logging, alerting, and
tracing</li>
<li>DODD focused on making data observability a first-class
consideration in the data engineering lifecycle</li>
</ul>
<h3 id="data-architecture">Data Architecture</h3>
<p>Types of data architectures</p>
<ul>
<li>Data warehouse: structured, includes the compute
<ul>
<li>A subject-oriented, integrated, nonvolatile, and time-variant
collection of data in support of mgmt decisions</li>
<li>Separates OLAP from OLTP</li>
<li>Centralized and organizes data</li>
<li>e.g., Snowflake, Amazon Redshift</li>
</ul></li>
<li>Data lake: unstructured, data can be in any format
<ul>
<li>Common during big data era</li>
<li>Data can be queried with MapReduce, Spark, Presto, etc…</li>
</ul></li>
<li>Data lakehouse: combines benefits from both, introduced by
Databricks
<ul>
<li>Includes control, data management and data structures</li>
<li>Still houses data in an object storage and supports a variety of
query and transformation engines</li>
</ul></li>
</ul>
<p>Modern Data Stack</p>
<ul>
<li>Instead of using monolithic toolsets, use cloud-based, plug and
play, off-the-shelf components</li>
<li>Includes data pipelines, storage, monitoring, etc…</li>
</ul>
<h2 id="data-generation-1">Data Generation</h2>
<h3 id="data-logs">Data Logs</h3>
<ul>
<li>Insert only: always retain the records, add a timestamp</li>
</ul>
<h3 id="messages-and-streams">Messages and Streams</h3>
<ul>
<li>Message queues and streaming platforms are often used
interchangeably</li>
<li>A message is raw data communicated across two or more systems
<ul>
<li>A message is normally sent through a message queue from a publisher
to a consumer</li>
<li>Once the message is delivered, it is removed from the queue</li>
</ul></li>
<li>A stream is an append only log of event records
<ul>
<li>You use streams when you care about what happened over many
events</li>
</ul></li>
</ul>
<h3 id="types-of-time">Types of time</h3>
<ul>
<li>Event time; event is generated</li>
<li>Ingestion time; event is ingested</li>
<li>Process time; event is processed</li>
</ul>
<h3 id="ways-of-ingesting-data">Ways of ingesting data</h3>
<ul>
<li>APIs
<ul>
<li>REST</li>
<li>GraphQL</li>
<li>WebHooks
<ul>
<li>Event based data transmission pattern</li>
<li>Data source can be an application backend, a web page, or a mobile
app</li>
</ul></li>
<li>RPC and gRPC: remote procedure call library; bidirectional exchange
of data over HTTP/2
<ul>
<li>Imposes more technical standards than REST</li>
</ul></li>
</ul></li>
<li>Message queues
<ul>
<li>Critical for decoupled microservices and event-driven
architectures</li>
<li>Need to keep in mind the frequency of delivery, message ordering and
scalability</li>
<li>Some issues with queues can be the order of the messages</li>
<li>Ideally, should be idempotent; outcome is the same after processing
it multiple times</li>
</ul></li>
<li>Event streaming platform
<ul>
<li>Produces data in an ordered log of records</li>
<li>Includes
<ul>
<li>Topics - collection of related events</li>
<li>Stream partitions</li>
</ul></li>
<li>Tend to be more fault tolerant because they’re distributed</li>
</ul></li>
</ul>
<h3 id="ingestion-undercurrents">Ingestion undercurrents</h3>
<ul>
<li>Security: how is the data secured and encrypted; do we have a VPN,
or an SSO?</li>
<li>Data management: who manages the data, what’s the quality, what if
the schema changes, is the data HIPAA?</li>
<li>DataOps: how will we know when there’s an issue, how are we
monitoring, is the schema conformant, what do we do if something bad
happens</li>
<li>Architecture: what if the system fails, what do we do if we lose
data, how available are the sources, who’s in charge of them</li>
<li>Orchestration: how often do we recieve the data, can we integrate
with the upstream application team</li>
<li>SWE: Can the code access with the right credentials, how do we
authenticate, how do we access (API, REST), can we parallelize the work,
how do we deploy code changes?</li>
</ul>
<h2 id="storage-1">Storage</h2>
<p>The raw ingredients are: disk drives, memory, networking and CUPU,
serialization, compression and caching</p>
<h3 id="raw-ingredients">Raw Ingredients</h3>
<ul>
<li>Magnetic Disk Drives
<ul>
<li>They can be slow but are cheap</li>
<li>They are constrained by disk transfer speed and rotational
latency</li>
<li>Storing info in parallel can make them faster</li>
<li>The max speed is 200-300 MB/s (20 hours for 30 TB)</li>
<li>Used widely because of how cheap they are</li>
<li>If we use multiple, the bottleneck becomes network and CPU</li>
</ul></li>
<li>Solid State Drives
<ul>
<li>Stores data in flash memory cells</li>
<li>They’re really fast (0.1 ms)</li>
<li>Used for OLTP Systems; allow Postgres</li>
<li>Cost 20-30 cents per GB (10x magnetic drives)</li>
<li>SSD can be used in some OLAP for caching</li>
</ul></li>
<li>RAM
<ul>
<li>Attached to CPU; stores code</li>
<li>It’s volatile, if it fails it gets reset</li>
<li>1000 times faster than SSD</li>
<li>$10/GB</li>
<li>Stores data in capacitors, needs to be refreshed over time</li>
</ul></li>
<li>Networking and CPU
<ul>
<li>Availability zones impact the storage access</li>
<li>Trade off in spreading data vs. keeping it under on zone</li>
</ul></li>
<li>Serialization
<ul>
<li>Flattening and packing data into standard format that reader will be
able to decode</li>
<li>e.g., Apache Parquet</li>
</ul></li>
<li>Compression
<ul>
<li>Increasing scan speed per disk</li>
</ul></li>
<li>Caching</li>
</ul>
<figure>
<img
src="https://assets-global.website-files.com/60d9fbbfcd9fcb40bad8aac3/63a4c83b39bde30ffb8aeec9_Screenshot2022-12-22at11.22.43AM_A1aXhW1Nn.png"
alt="caching" />
<figcaption aria-hidden="true">caching</figcaption>
</figure>
<h3 id="data-storage-systems">Data Storage Systems</h3>
<p>BASIC - Basis of eventual consistency - Basically available;
consistency is not guaranteed - Soft state - state of transaction is
fuzzy - Eventual consistency - at some point, reading data will return
consistent values</p>
<p>Why do BASIC? It allows us to use large-scale distributed systems.
i.e., scale horizontally.</p>
<p>When do we do strong consistency? When we can tolerate longer query
times but want the correct data every time.</p>
<p>Types of file storage - Cloud filesystem services: e.g., Amazon
Elastic File System; exposed through the NFS4 protocol - Block storage:
type of raw storage provided by SSDs and magnetic disks; standard for
VMs - RAID: redundant array of independent disks. Controls multiple
disks to improve data durability, enhance performance, and combine
capacity - Cloud virtualized block storage: similar to storage area
network (block storage over a network). They separate data from the host
server. Can persist the data when an EC2 instance shuts down. Highly
scalable. - Object storage - contains objects of all shapes and sizes.
Amazon S3, Azure Blob Storage. We lose a lot of the writing flexibility
with file storage on a local disk. Objects are written once as a stream
of bytes. To change data or append you must rewrite the full object. -
Object stores are the gold standard for data lakes. - Object stores are
key value stores. - We have bucket names and item values - Cache and
Memory Storage systems: e.g., memcached, a key-value store desgined for
caching db query results, api call repsonses and more. Redis, a
key-value store, supports more complex data types. Can tolerate a small
amount of data loss. - Streaming storage: stored data is temporal. Kafka
now allows long data retention.</p>
<p>Data abstractions - Data warehouse: standard olap architecture. -
Data lake: massive store where data was retained raw and unprocessed.
Originally built on Hadoop systems. Now we’re separating compute and
storage. - Data lakehouse: combines aspects of the data warehouse and
data lake. Stores data in object storage. Adds robust table and schema
support for incremental updates and deletes. Has file management layer
with data mgmt and transf tools. Easier to send data when other tools
can directly read from the object</p>
<h2 id="data-ingestion">Data Ingestion</h2>
<p>It’s the process of moving data from one place to the other. A data
pipeline is the combination of architecture, systems, and processed that
move data through the stages of the DE lifecycle.</p>
<p>Considerations: - Bounded vs. unbounded - Discrete batches or stream
of consciousness (stream of data) - Frequency - Batch, micro batch or
real time (or streaming) - Sync or async - With sync, the source,
ingestion, and destination have complex dependencies and are tightly
coupled - With async, dependencies can operate at the level of
individual events; like microservices. - Serialization or
deserialization - Encoding the data from the source and preparing data
structures for transmission - Throughput and scalability - Will your
ingestion be able to keep up with a sudden influx of backlogged data -
Reliability and durability - Reliability entails high uptime and proper
failover for ingestion systems - Durability entails making sure data
isn’t lost or corrupted - Need to build redundancy and self healing
based on the impact and cost of losing data - Payload - Data type - Data
shape (json, tabular, unstructured) - Schema - Push vs. pull vs. poll
patterns - If there’s a change, pull (polling) - Push from source to
destination - Destination pulls from source</p>
<p><strong>Ways to ingest data</strong> - Direct DB connection - OBDC or
JBDC via API - Change Data Capture - Ingesting changes from a source
system - Batch oriented CDC; every 24 hours update changes in records -
Continuous CDC; capture all table history and support near real time
data ingestion - CDC can consume database resoures, CPU time, network
bandwitdth, disk bandwidth - APIs - Increasingly popular - Common with
SaaS platforms - Message Queues and Event-Streaming - Managed data
connectors - Managed by vendor - SFTP and SCP; run over SSH connection -
Webhooks; aka reverse APIs - The data provider defines an API request
spec; iit makes the API calls rather than receiving them - The consumer
needs to provide an endpoint for the provider to call - Can be brittle,
difficult to maintain and inefficient - Web scraping</p>
<p><strong>Undercurrent of data ingestion</strong> - DataOps - Need to
monitor uptime, latency, and data volumes - How will you respond if
ingestion job fails - Monitoring should be there from the beginning - Is
there proper testing and deployment automation? Can you roll it back? -
Data quality - Data can regress whenever and without warning - Need to
monitor statistics, spikes, etc… - Implement logs to capture history,
checks, exception handling - Can you do some statistical data testing?
averages, nulls, outliers?</p>
<h2 id="data-transformation">Data Transformation</h2>
<p>The lifetime of a SQL query is: - Query issued - Parsing and
conversion to bytecode - Query planning and optimization - Query
execution - Return results</p>
<p>A data model represents the way data relates to the real world. How
it must be organized to reflect the organization’s processes,
definitions, workflow and logic.</p>
<p>Types of normalization (Codd normal forms) - Denormalized - no
normalization, nested and redundant data allowed - First normal form -
each column is unique and has a single value; table has a unitque
primary key - Second normal form - 1NF plus partial dependencies are
removed - Third normal form - 2NF plus each table contains only relevant
fields related to its primary key and has no dependencies</p>
<p>Data modeling - Star schema: data is modeled with two general types
of tables: facts and dimentions - Facts is a table of numbers. e.g.,
order ID, customer key, date key, gross sale amount - Dimension tables
as qualitative data referencing a fact; smaller than fact tables. e.g.,
datekey, year, quarter, month, day of week.</p>
<p>Update patterns - Truncate and reload; wipe old data and reload -
Insert only; insert new records without changing or deleting old
records. - Delete; deletes are more expensive than inserts in columnar
system. Insert deletion inserts a new record with a deleted flag.
Queries can get more complicated - Upsert/merge; upserting takes a set
of source records and looks for matches using a primary key. When it
matches, it updates the record, otherwise it inserts the new record.</p>
<h2 id="serving-data">Serving data</h2>
<p>What a data engineer should know about ML: - When to use which
models? - How to wrangle data for unstructured and structured data? -
How to encode data for various types - Batch vs. online learning? - When
to use AutoML vs. handcrafting an ML model? - Classification
vs. regression - When to train locally, on a cluster, or at the edge -
What are data cascades?</p>
<h2 id="new-trends-in-data-engineering">New trends in data
engineering</h2>
<ul>
<li>Application stacks will be data stacks. Applications will integrate
real time automation and decision making powered by streaming pipelines
and ML</li>
<li>We will have a live data stack; application and source systems will
feed data to low latency fast query processing DBs, which will feed data
to report, analytics, and ML</li>
<li>We will have more streaming pipelines and real time analytical DBs
<ul>
<li>Data will move in a contiguous flow moving away from batch
ingestion</li>
<li>Shift away from ELT and more to stream transform and load.</li>
<li>This live data stack will be powered by OLAP DBs built for
streaming</li>
</ul></li>
<li>We will have tighter feedback loops between applications and
ML.</li>
</ul>


</main>
<footer><hr />
<span style='float:right'>Edited on 2023-11-01</span><b>Mente</b> © 2023 — 
</footer>
</body></html>
