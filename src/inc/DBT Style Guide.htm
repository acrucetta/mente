<h1>DBT Style Guide</h1>
<h2>Style Guide Sources</h2>
<ul>
<li><a href="https://github.com/dbt-labs/corp/blob/main/dbt_style_guide.md">Official DBT Labs Style Guide</a></li>
</ul>
<h2>Design Principles</h2>
<ul>
<li><strong>Fail fast</strong>: Reduce the time it takes for your developers to realize that a code defect exists. You can achieve this by using running linting and data quality tests as part of the CI pipeline. Reduce the time it takes to run the CI pipeline so that developers can get faster feedback cycles.</li>
<li><strong>Fail left</strong>: encourage your developers to write dbt tests starting from left (dbt sources) to right (data mart dbt models). The earlier in the pipeline you can detect errors and fail the DAG, the easier it is to fix problems. It is very difficult to troubleshoot problems at the end of the DAG that is caused by an upstream failure at the start of the DAG.</li>
<li><strong>Distributed ownership</strong>: Each dbt model should have an owner (a person). The owner is the person that reviews the PR. The owner would typically be an analytics engineer for a particular domain or business function (a person that understands the business/domain context and can translate that to SQL code). They would be best placed to review the code. See this <a href="https://www.thoughtworks.com/en-au/insights/blog/data-mesh-its-not-about-tech-its-about-ownership-and-communication">blog on domain ownership</a>.</li>
</ul>
<h2>DBT Folder Structure Best Practices</h2>
<ul>
<li><strong>Staging</strong>
<ul>
<li>Model Naming:
<ul>
<li>stg_[source_system]__[source_table]</li>
</ul>
</li>
<li>Rules
<ul>
<li>✅ Renaming</li>
<li>✅ Type casting</li>
<li>✅ Basic computations (e.g. cents to dollars)</li>
<li>✅ Categorizing (using conditional logic to group values into buckets or booleans, such as in the case when statements above)</li>
<li>❌ Joins — the goal of staging models is to clean and prepare individual source-conformed concepts for downstream usage. We're creating the most useful version of a source system table, which we can use as a new modular component for our project. In our experience, joins are almost always a bad idea here — they create immediate duplicated computation and confusing relationships that ripple downstream — there are occasionally exceptions though (refer to base models for more info).</li>
<li>❌ Aggregations — aggregations entail grouping, and we're not doing that at this stage. Remember - staging models are your place to create the building blocks you’ll use all throughout the rest of your project — if we start changing the grain of our tables by grouping in this layer, we’ll lose access to source data that we’ll likely need at some point. We just want to get our individual concepts cleaned and ready for use, and will handle aggregating values downstream.</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="language-sql">-- stg_stripe__payments.sql

with

source as (

    select * from {{ source('stripe','payment') }}

),

renamed as (

    select
        -- ids
        id as payment_id,
        orderid as order_id,

        -- strings
        paymentmethod as payment_method,
        case
            when payment_method in ('stripe', 'paypal', 'credit_card', 'gift_card') then 'credit'
            else 'cash'
        end as payment_type,
        status,

        -- numerics
        amount as amount_cents,
        amount / 100.0 as amount,

        -- booleans
        case
            when status = 'successful' then true
            else false
        end as is_completed_payment,

        -- dates
        date_trunc('day', created) as created_date,

        -- timestamps
        created::timestamp_ltz as created_at

    from source

)

select * from renamed
</code></pre>
<ul>
<li><strong>Intermediate</strong>
<ul>
<li>Folders
<ul>
<li>✅ Subdirectories based on business groupings. Much like the staging layer, we’ll house this layer of models inside their own intermediate subfolder. Unlike the staging layer, here we shift towards being business-conformed, splitting our models up into subdirectories not by their source system, but by their area of business concern.</li>
</ul>
</li>
<li>File names
<ul>
<li>✅ int_[entity]s_[verb]s.sql - the variety of transformations that can happen inside of the intermediate layer makes it harder to dictate strictly how to name them. 
<ul>
<li>The best guiding principle is to think about verbs (e.g. pivoted, aggregated_to_user, joined, fanned_out_by_quantity, funnel_created, etc.) in the intermediate layer. In our example project, we use an intermediate model to pivot payments out to the order grain, so we name our model int_payments_pivoted_to_orders. </li>
<li>It’s easy for anybody to quickly understand what’s happening in that model, even if they don’t know SQL. That clarity is worth the long file name. It’s important to note that we’ve dropped the double underscores at this layer. In moving towards business-conformed concepts, we no longer need to separate a system and an entity and simply reference the unified entity if possible. In cases where you need intermediate models to operate at the source system level (e.g. int_shopify__orders_summed, int_core__orders_summed which you would later union), you’d preserve the double underscores. </li>
</ul>
</li>
<li>Some people like to separate the entity and verbs with double underscores as well. That’s a matter of preference, but in our experience, there is often an intrinsic connection between entities and verbs in this layer that make that difficult to maintain.</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="language-sql">-- int_payments_pivoted_to_orders.sql

{%- set payment_methods = ['bank_transfer','credit_card','coupon','gift_card'] -%}

with

payments as (

   select * from {{ ref('stg_stripe__payments') }}

),

pivot_and_aggregate_payments_to_order_grain as (

   select
      order_id,
      {% for payment_method in payment_methods -%}

         sum(
            case
               when payment_method = '{{ payment_method }}' and
                    status = 'success'
               then amount
               else 0
            end
         ) as {{ payment_method }}_amount,

      {%- endfor %}
      sum(case when status = 'success' then amount end) as total_amount

   from payments

   group by 1

)

select * from pivot_and_aggregate_payments_to_order_grain
</code></pre>
<ul>
<li><strong>Mart</strong>
<ul>
<li>✅ Group by department or area of concern. If you have fewer than 10 or so marts you may not have much need for subfolders, so as with the intermediate layer, don’t over-optimize too early. If you do find yourself needing to insert more structure and grouping though, use useful business concepts here. In our marts layer, we’re no longer worried about source-conformed data, so grouping by departments (marketing, finance, etc.) is the most common structure at this stage.</li>
<li>✅ Name by entity. Use plain English to name the file based on the concept that fo*rms the grain of the mart customers, orders. Note that for pure marts, there should not be a time dimension (orders_per_day) here, that is typically best captured via metrics.</li>
<li>❌ Build the same concept differently for different teams. finance_orders and marketing_orders is typically considered an anti-pattern. There are, as always, exceptions — a common pattern we see is that, finance may have specific needs, for example reporting revenue to the government in a way that diverges from how the company as a whole measures revenue day-to-day. Just make sure that these are clearly designed and understandable as separate concepts, not departmental views on the same concept: tax_revenue and revenue not finance_revenue and marketing_revenue.</li>
</ul>
</li>
</ul>
<pre><code class="language-sql">-- orders.sql

with

orders as  (

    select * from {{ ref('stg_jaffle_shop__orders' )}}

),

order_payments as (

    select * from {{ ref('int_payments_pivoted_to_orders') }}

),

orders_and_order_payments_joined as (

    select
        orders.order_id,
        orders.customer_id,
        orders.order_date,
        coalesce(order_payments.total_amount, 0) as amount,
        coalesce(order_payments.gift_card_amount, 0) as gift_card_amount

    from orders

    left join order_payments on orders.order_id = order_payments.order_id

)

select * from orders_and_payments_joined
</code></pre>
<h2>CI/CD Best Practices</h2>
<p><strong>Continuous integration (CI)</strong></p>
<ul>
<li><em><strong>Run code linting tests</strong></em>:
<ul>
<li>Run code linting tests (e.g. <a href="https://www.sqlfluff.com/">sqlfluff</a>) to validate that code is written in a readable format. Only if code linting passes, can code be merged to the main branch.</li>
</ul>
</li>
<li><em><strong>Run data quality tests in a temporary development environment:</strong></em>
<ul>
<li><strong>Option 1</strong>: Create a development environment with prod-like data as part of the CI pipeline.
<ul>
<li>If you are using Snowflake or Databricks, this can easily be done via zero-copy clone (snowflake) or shallow clone (databricks).</li>
<li>Perform dbt run against the cloned database, and then run the transformation data quality tests to validate that the output passes the tests. A dbt command you can use to do both run and test is &quot;dbt build&quot;.</li>
<li>Only build modified models and their immediate child e.g. &quot;dbt build -m state:modified+1&quot;. By running and testing only modified models and their immediate child, the run takes shorter, and you achieve faster feedback cycles.</li>
<li>Note: I wouldn't try to execute this approach unless your OLAP warehouse or lakehouse supports shallow clone or zero-copy clone.</li>
</ul>
</li>
<li><strong>Option 2</strong>: Use state selector and defer flag. For example &quot;dbt build -m state:modified+1 --defer --state /path/to/prod_manifest_folder --target dev&quot;
<ul>
<li>The above command will do the following:</li>
<li>Only build modified models and their immediate child in the dev environment.</li>
<li>Run the modified models by fetching upstream data from the actual prod database. dbt does this because you have specified --defer and provided it with the --state manifest_folder. See docs on <a href="https://docs.getdbt.com/reference/node-selection/defer">defer here</a>.</li>
<li>This is a pattern that has been blogged about in more detail <a href="https://discourse.getdbt.com/t/how-we-sped-up-our-ci-runs-by-10x-using-slim-ci/2603">here</a>.</li>
</ul>
</li>
</ul>
</li>
<li><em><strong>Use CI tooling:</strong></em>
<ul>
<li>In terms of tooling - almost any devops tool will do the job. Examples: buildkite, github actions, gitlab pipelines, jenkins, octopus deploy, too many to name. The tool just has to allow you to specify steps (typically in YAML) that will run on an agent (a virtual machine that is either hosted for you, or you have to host yourself).</li>
</ul>
</li>
</ul>
<p><strong>Continuous deployment (CD)</strong></p>
<ul>
<li>Create two or more target environments e.g. pre-prod, and prod. For each environment (starting with pre-prod then prod):
<ul>
<li>Deploy all code to your orchestration tool e.g. dbt cloud, cron, airflow, dagster, prefect, etc.</li>
<li>Trigger the orchestration pipeline to perform &quot;dbt build&quot; which runs both dbt run and dbt test. If you can't do dbt build, then do dbt run &gt;&gt; dbt test.</li>
<li>If the pipeline execution succeeds and data quality test passes, then the deployment was successful.</li>
</ul>
</li>
<li>Only run the deployment to prod if the deployment pre-prod is successful.</li>
</ul>
<h2>Developer Guide</h2>
<h3>Pre-commit check-list</h3>
<ul>
<li>[] Run code linting tests (e.g. <a href="https://www.sqlfluff.com/">sqlfluff</a>) to validate that code is written in a readable format. Only if code linting passes, can code be merged to the main branch.</li>
<li>[] Run dbt run and dbt test locally to validate that the code passes all tests. Only if the code passes all tests, can code be merged to the main branch.</li>
<li>[] Run dbt docs generate locally to validate that the code generates documentation. Only if the code generates documentation, can code be merged to the main branch.</li>
</ul>
