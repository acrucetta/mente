<h2>Notes on Operating Systems in 3 Easy Pieces</h2>
<h3>Anki Questions</h3>
<h4>System Calls</h4>
<p><strong>What does wait() do?</strong> The parent process calls wait()
to delay its execution until the child finishes executing. When the
child is done, wait() returns to the parent.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> main<span class="op">()</span> <span class="op">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> rc <span class="op">=</span> fork<span class="op">();</span> <span class="co">// Fork a new process</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>rc <span class="op">&lt;</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Fork failed; exit</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        fprintf<span class="op">(</span>stderr<span class="op">,</span> <span class="st">&quot;fork failed</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        exit<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="cf">if</span> <span class="op">(</span>rc <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Child process</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        printf<span class="op">(</span><span class="st">&quot;hello, I am child (pid:</span><span class="sc">%d</span><span class="st">)</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> <span class="op">(</span><span class="dt">int</span><span class="op">)</span>getpid<span class="op">());</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Parent process</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> wc <span class="op">=</span> wait<span class="op">(</span>NULL<span class="op">);</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        printf<span class="op">(</span><span class="st">&quot;hello, I am parent of </span><span class="sc">%d</span><span class="st"> (wc:</span><span class="sc">%d</span><span class="st">) (pid:</span><span class="sc">%d</span><span class="st">)</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> rc<span class="op">,</span> wc<span class="op">,</span> <span class="op">(</span><span class="dt">int</span><span class="op">)</span>getpid<span class="op">());</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>What does fork() do?</strong> The fork() system call is used
to create a new process</p>
<p><strong>What does exec() do?</strong> This system call is useful when
you want to run a program that is different from the calling program.
For example, calling fork() in p2.c is only useful if you want to keep
running copies of the same program</p>
<pre><code>
17 char *myargs[3];
18 myargs[0] = strdup(&quot;wc&quot;); // program: &quot;wc&quot; (word count)
19 myargs[1] = strdup(&quot;p3.c&quot;); // argument: file to count
20 myargs[2] = NULL; // marks end of array
21 execvp(myargs[0], myargs); // runs word count</code></pre>
<p>How does pipe() work? What are the main components? Why do we need
different modes of execution for a CPU? What is a trap, what is a trap
table? How can the OS effectively switch between processes? What do we
do when an OS process goes rogue? How does the CPU prevent it?</p>
<p><strong>What is a context switch?</strong></p>
<p>A context switch is conceptually simple: all the OS has to do is
<strong>save a few register values for the currently-executing
process</strong> (onto its kernel stack, for example) and
<strong>restore a few for the soon-to-be-executing process</strong>
(from its kernel stack).</p>
<p>By doing so, the OS thus <strong>ensures that when the
return-from-trap instruction is finally executed, instead of returning
to the process that was running, the system resumes execution of another
process.</strong></p>
<p><strong>How does the OS "baby proof" the CPU? What are the
equivalents?</strong></p>
<p>By during boot time setting up the trap handlers and starting an
interrupt timer, and then by only running processes in a restricted
mode. By doing so, the OS can feel quite assured that processes can run
efficiently, only requiring OS intervention to perform privileged
operations or when they have monopolized the CPU for too long and thus
need to be switched out.</p>
<p><strong>What is a timer interrupt? Why do we need it?</strong></p>
<p>The addition of a timer interrupt gives the OS the ability to run
again on a CPU even if processes act in a non-cooperative fashion. Thus,
this hardware feature is essential in helping the OS maintain control of
the machine.</p>
<h3>Scheduling</h3>
<p>What are the key assumptions made in the simple scheduling
examples?</p>
<ol type="1">
<li>Each job runs for the same amount of time.</li>
<li>All jobs arrive at the same time.</li>
<li>Once started, each job runs to completion.</li>
<li>All jobs only use the CPU (no I/O).</li>
<li>The run-time of each job is known.</li>
</ol>
<p>What are the key metrics for evaluating scheduling algorithms?</p>
<ul>
<li>Response time: time of first run - time of arrival</li>
<li>Turnaround time: time of completion - time of arrival</li>
</ul>
<p>What are the main types of simple scheduling policies mentioned and
what do they optimize for?</p>
<ul>
<li>First-in First-out (FIFO)</li>
<li>Shortest Job First (SJF): Optimizes turnaround time</li>
<li>Shortest Time-to-Completion First (STCF): Optimizes turnaround
time</li>
<li>Round Robin (RR): Optimizes response time</li>
</ul>
<p>What is the Multi-Level Feedback Queue (MLFQ) and what does it
optimize for?</p>
<p>MLFQ is a scheduling algorithm that varies job priority based on
observed behavior. It optimizes for turnaround time by running shorter
jobs first, while also trying to provide good responsiveness for
interactive users. It learns about processes as they run and uses their
history to predict future behavior.</p>
<p>What are the first two rules of the MLFQ?</p>
<ul>
<li>Rule 1: If Priority(A) &gt; Priority(B), A runs (B doesn’t).</li>
<li>Rule 2: If Priority(A) = Priority(B), A &amp; B run in RR.</li>
</ul>
<p>How does MLFQ handle newly arriving jobs and jobs that use their
entire time slice?</p>
<ul>
<li>Rule 3: When a job enters the system, it is placed at the highest
priority (the topmost queue).</li>
<li>Rule 4a: If a job uses up an entire time slice while running, its
priority is reduced (i.e., it moves down one queue).</li>
</ul>
<h2>Lectures</h2>
<h3>Lecture 1</h3>
<p>CPUs give us the illusion of running a program.</p>
<p>CPUs give us the abstraction of a process. A running program.</p>
<p>Processes have a private memory and "address space"; they also have
registers, and a stack pointer.</p>
<p>The computer can run programs in Kernel Mode (OS) and User Mod (user
program). Limited number of things.</p>
<p>Special services to operate on the OS can be called system calls.</p>
<p>Boot Time:</p>
<ul>
<li>OS starts in Kernel mode; it tells the hardware where to jump to
when the user specifies a trap #</li>
<li>We set up trap handlers in the OS; tell H/W where the trap handlers
are in OS memory</li>
</ul>
<p>Key Questions</p>
<ul>
<li>What if OS wants to run a operation that's higher</li>
<li>What if the OS wants to stop A, then run B</li>
</ul>
<h3>Lecture 2</h3>
<p>A CPU is basically a while loop that fetches the program counter.
Figures out which instruction it is. And executes it.</p>
<pre><code>while (1) {
  fetch(pc)
  decode
  increment pc
  (before executing)
    check permission (kernel or os)
    if not ok raise exception
      OS can get involved
  execute (can change pg)
    process interrupts
}</code></pre>
<p>Before it executes; it checks permission. Is this instrunction OK to
execute.</p>
<ul>
<li>It basically checks permissions; if the instruction is OK to execute
do it. If not raise exception.</li>
</ul>
<p>A timer interrupt runs every once in a while. It makes the CPU decide
if it wants to run the current program or switch.</p>
<p>CPU virtualization mechanisms.</p>
<p>At boot time: OS runs first at priviledged mode (kernel mode).</p>
<ul>
<li>Install handlers (tell hardware what to run)</li>
<li>Tell H/W what code to run on exception/interrupt traps</li>
<li>Initialize timer interrupt</li>
<li>Ready ro run user programs</li>
</ul>
<p>Timeline:</p>
<ul>
<li>A trap instruction:
<ul>
<li>Transitions us from user mode to kernel mode</li>
<li>Jumps into OS: target trap handlers</li>
<li>Save register state (to execute later)</li>
<li>Return from trap (opposite of above)</li>
</ul></li>
</ul>
<p>The OS must track different user processes. It uses a <strong>process
list</strong></p>
<ul>
<li>Per-process info:
<ul>
<li>state: ready, running</li>
</ul></li>
</ul>
<h2>A big problem: some operations are slow; the OS needs to do what to
do.</h2>
<h2>Book Notes</h2>
<h3>Scheduling: Introduction</h3>
<p><strong>Key assumptions:</strong></p>
<ol type="1">
<li>Each job runs for the same amount of time.</li>
<li>All jobs arrive at the same time.</li>
<li>Once started, each job runs to completion.</li>
<li>All jobs only use the CPU (i.e., they perform no I/O) 5. The
run-time of each job is known.</li>
</ol>
<p>Types of scheduling:</p>
<ul>
<li><strong>First-in first out</strong></li>
<li><strong>Shortest job fist (SJF) - optimizes turnaround time</strong>
<ul>
<li>This new scheduling discipline is known as Shortest Job First (SJF),
and the name should be easy to remember because it describes the policy
quite completely: it runs the shortest job first, then the next
shortest, and so on.</li>
</ul></li>
<li><strong>Shortest time to completion first (STCF) - optimizes
turnaround time</strong>
<ul>
<li>Any time a new job enters the system, the STCF scheduler determines
which of the remaining jobs (including the new job) has the least time
left, and schedules that one. Thus, in our example, STCF would preempt A
and run B and C to completion</li>
</ul></li>
<li><strong>Round robin - optimizes response time</strong>
<ul>
<li>The basic idea is simple: instead of running jobs to completion, RR
runs a job for a time slice (sometimes called a scheduling quantum) and
then switches to the next job in the run queue.</li>
<li>The shorter the time slice the better; deciding on this length is a
trade-off</li>
<li>Any policy (such as RR) that is fair, i.e., that evenly divides the
CPU among active processes on a small time scale, will perform poorly on
metrics such as turnaround time</li>
</ul></li>
</ul>
<p><strong>Metrics</strong>:</p>
<ul>
<li>Response time: time of first run - time of arrival</li>
<li>Turnaround time: time of completion - time of arrival</li>
</ul>
<h3>Scheduling: Multi-Level Feedback Queue</h3>
<p>The Multi-level Feedback Queue (MLFQ) sched- uler was first described
by Corbato et al. in 1962 [C+62] in a system known as the Compatible
Time-Sharing System (CTSS), and this work, along with later work on
Multics, led the ACM to award Corbato its highest honor, the Turing
Award.</p>
<p>It optimizes for turnaround time. Done by running shorter jobs first.
It tries to make the system feel responsive to interactive users.
(minimizing staring at the screen and waiting)</p>
<p><em>Key Question: How do we schedule without perfect
knowledge?</em></p>
<p><strong>MLFQ</strong> varies the priority of a job based on its
observed behavior. If, for example, a job repeatedly relinquishes the
CPU while waiting for input from the keyboard, MLFQ will keep its
priority high, as this is how an interactive process might behave. If,
instead, a job uses the CPU intensively for long periods of time, MLFQ
will reduce its priority. In this way, MLFQ will try to learn about
processes as they run, and thus use the history of the job to predict
its future behavior.</p>
<p>Thus, we arrive at the first two basic rules for MLFQ: • Rule 1: If
Priority(A) &gt; Priority(B), A runs (B doesn’t). • Rule 2: If
Priority(A) = Priority(B), A &amp; B run in RR.</p>
<p><strong>Changing Priorities</strong></p>
<ul>
<li>Rule 3: When a job enters the system, it is placed at the highest
priority (the topmost queue).</li>
<li>Rule 4a: If a job uses up an entire time slice while running, its
priority is reduced (i.e., it moves down one queue).</li>
<li>Rule4b: If it gives up the CPU before the time slice is up; stay at
same priority level</li>
</ul>
<p>Key issues:</p>
<ul>
<li>Starvation; what if we have a LOT of interactive jobs; long-running
CPU jobs will never finish</li>
<li>Gaming the scheduler: sneaky trick to get more time than its fair;
e.g., issue a quick IO before the time slice is up</li>
</ul>
<p><strong>Boosting priorities</strong></p>
<ul>
<li>Rule 5: After some time period S, move all the jobs in the system to
the topmost queue.</li>
</ul>
<p>It guarantees the processes won't starve by sitting in the top queue.
It also shares the CPU with other high priority jobs. If a job becomes
interactive it will receive a proper priority boost.</p>
<p><strong>Add Accounting</strong></p>
<ul>
<li>Rule 4: Once a job uses up its time allotment at a given level (re-
gardless of how many times it has given up the CPU), its priority is
reduced (i.e., it moves down one queue).</li>
</ul>
<p><em>MLFQ is interesting because instead of demanding a priori
knowledge of the nature of a job, it instead observes the execution of a
job and pri- oritizes it accordingly.</em></p>
<h3>Scheduling: Proportional Share</h3>
<p>In this case we want to optimize for each CPU process to obtain a
certain percentage of CPU time.</p>
<p>The key question is how can we degin a scheduler to share the CPU in
a proportional manner.</p>
<p>To solve for this, we can use tickets and randomness. We pull a
number from 0 to 99; assuming process A holds 0 through 74 and B 75
through 99. The winning ticket determies whether A or B runs.</p>
<p>It's not perfect but it will get us to a rough percentage over
time.</p>
<p>Ticket Mechanisms</p>
<ul>
<li>Ticket currency: allow users to allocate tickets among their own
jobs..</li>
<li>Ticket transfer: in client/server settings, allow the client to send
some tickets to the server</li>
<li>Ticket inflation: a process can temporarily raise or lower the
number of ticket it owns. If a process needs more CPU time it can raise
the ticket value to reflect that need to the system</li>
</ul>
<p>Implementation:</p>
<ul>
<li>We can use a linked list</li>
</ul>
<p>head -&gt; job A, tix 100 -&gt; job b, tix 50 -&gt; job c, tix 250
-&gt; null</p>
<p>Keep going over the list with a given tix number and once you go over
pick that job.</p>
<p>Deterministic Scheduling:</p>
<ul>
<li>We can also use "stride scheduling" which is basically representing
each job's stride as the inverse of the number of tickets they hold</li>
<li>At each point we pick the job with the lowest stride number. Then
increase its number. The jobs with the lowest stride (highest tickets)
will run more often</li>
<li>The downside is we have to maintain global state</li>
</ul>
<p>Proportional-share scheduleres are more useful in domains that are
easier to solve. E.g., a virtualized data center where you can assign
CPU cycles to given VMs.</p>
<h2>Virtualization</h2>
<p>Every address generated by a user program is a virtual address. The
OS gives an illusion that it has a large and private memory. It wants to
give programmers the illusion that you have a long contiguous address
space to put the code into data.</p>
<h3>Abstraction: Address Spaces</h3>
<p>Main Question:</p>
<ul>
<li>How can the OS build this abstraction of a private, potentially
large address space for multiple running processes (all sharing memory)
on top of a single, physical memory?</li>
</ul>
<p>Isolation is key in building reliable systems. The OS wants to run
programs independent of each other. If one fails, it wants to prevent
the other ones from failing. Some OS even wall off the OS from others
with microkernels. (compared with monolithic kernel designs)</p>
<p><strong>Goals of virtualizing memory:</strong></p>
<ul>
<li>Transparency - memory visible to the running program</li>
<li>Efficiency - fast processes using little memory; use hardware
support</li>
<li>Protection - protect processes from one another</li>
</ul>
<h3>Memory API</h3>
<p>Correct memory management has been such a problem, in fact, that many
newer languages have support for automatic memory manage- ment. In such
languages, while you call something akin to malloc() to allocate memory
(usually new or something similar to allocate a new object), you never
have to call something to free space; rather, a garbage collector runs
and figures out what memory you no longer have references to and frees
it for you.</p>
<p>Common issues:</p>
<ul>
<li>Forgetting to allocate memory</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>src <span class="op">=</span> <span class="st">&quot;hello&quot;</span><span class="op">;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>dst<span class="op">;</span> <span class="co">// oops! unallocated </span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>strcpy<span class="op">(</span>dst<span class="op">,</span> src<span class="op">);</span> <span class="co">// segfault and dieA</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>src <span class="op">=</span> <span class="st">&quot;hello&quot;</span><span class="op">;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>dst <span class="op">=</span> <span class="op">(</span><span class="dt">char</span> <span class="op">*)</span> malloc<span class="op">(</span>strlen<span class="op">(</span>src<span class="op">)</span> <span class="op">+</span> <span class="dv">1</span><span class="op">);</span> </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>strcpy<span class="op">(</span>dst<span class="op">,</span> src<span class="op">);</span> <span class="co">// work properly</span></span></code></pre></div>
<ul>
<li>Not allocating enough memory</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>src <span class="op">=</span> <span class="st">&quot;hello&quot;</span><span class="op">;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>dst <span class="op">=</span> <span class="op">(</span><span class="dt">char</span> <span class="op">*)</span> malloc<span class="op">(</span>strlen<span class="op">(</span>src<span class="op">));</span> <span class="co">// too small! </span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>strcpy<span class="op">(</span>dst<span class="op">,</span> src<span class="op">);</span> <span class="co">// work properly</span></span></code></pre></div>
<ul>
<li>Forgetting to initialize allocated memory</li>
<li>Forgetting to free memory</li>
<li>Freeing memory before you're done aka dangling pointers</li>
<li>Double freeing memory</li>
</ul>
<p>Good tools to prevent issues: check out both purify [HJ92] and
valgrind [SN05]; both are excellent at helping you locate the source of
your memory-related problems.</p>
<h2>Project 1B</h2>
<p>What are system calls?</p>
<ul>
<li>A call between you and the machine, you call the OS and ask him to
do stuff for you</li>
</ul>
<h3>Virtual Memory: Address Translation</h3>
<p>Address translation implies mapping the virtual addresses to the
physical space. This happens in the kernel space since its a priviledged
operation. If we don't find an address we might get a segmentation
fault.</p>
<p>The most popular method is dynamic relocation which is:</p>
<p><code>physical address = virtual address + base</code></p>
<p>The hardware provides base and bound registers. The hardware needs to
check whether the address is valid. This is done by the base and bounds
register.</p>
<p>The OS also needs to know which areas of memory are free through the
"free list".</p>
<pre><code>ARG seed 0
ARG address space size 1k
ARG phys mem size 64k

Base-and-Bounds register information:

  Base   : 0x00008000 (decimal 32768)
  Limit  : 16384

Virtual Address Trace
  VA  0: 0x00000360 (decimal:  864) --&gt; VALID: 0x00008360 (decimal: 33632)
  VA  1: 0x00000308 (decimal:  776) --&gt; VALID: 0x00008308 (decimal: 33544)
  VA  2: 0x000001ae (decimal:  430) --&gt; VALID: 0x000081ae (decimal: 33198)
  VA  3: 0x00000109 (decimal:  265) --&gt; VALID: 0x00008109 (decimal: 33033)
  VA  4: 0x0000020b (decimal:  523) --&gt; VALID: 0x0000820b (decimal: 33291)</code></pre>
<h2>Segmentation</h2>
<p>Key question: how do we support a large address space with many gaps
between the stack and the heap.</p>
<p>If we segment, we can place each one of the segments in diff parts of
the physical memory avoiding unused virtual address space.</p>
<p>The infamous term segmentation fault arises from a memory access on a
segmented machine to an illegal address.</p>
<p>We can have coarse grained and fine-grained segmentation.</p>
<p>There are many algorithms to minimize external framgentation. Which
means there's not one perfect solution.</p>
<p>There are issues when we try allocating variable-sized segments in
memory. The main one is external fragmentation. Because they get chopped
up it can be hard to allocate the right amount of memory. We also still
may have large segments of unused memory. The heap still needs the full
allotted space to work.</p>
<pre><code>ARG seed 2
ARG address space size 128
ARG phys mem size 1024k

Segment register information:

  Segment 0 base  (grows positive) : 0x00000000 (decimal 0)
  Segment 0 limit                  : 20

  Segment 1 base  (grows negative) : 0x00000200 (decimal 512)
  Segment 1 limit                  : 20

Virtual Address Trace
  VA  0: 0x0000007a (decimal:  122) --&gt; VALID in SEG1: 0x000001fa (decimal:  506)
  VA  1: 0x00000079 (decimal:  121) --&gt; VALID in SEG1: 0x000001f9 (decimal:  505)
  VA  2: 0x00000007 (decimal:    7) --&gt; VALID in SEG0: 0x00000007 (decimal:    7)
  VA  3: 0x0000000a (decimal:   10) --&gt; VALID in SEG0: 0x0000000a (decimal:   10)
  VA  4: 0x0000006a (decimal:  106) --&gt; SEGMENTATION VIOLATION (SEG1)</code></pre>
<p>In the sample address space above. We have 128 bye address spaces
mapped two two segments. Segment 0 is the heap and segment 1 the stack
(grows up). We can only map values it seems that grow from 0 to 20 or
from 128 to 108. Therefore the valid addresses are 0-20 and 108-128. 106
causes a segment violation and it can't be mapped to our memory.</p>
<h3>Free-Space Management</h3>
<p>Key question: how do we manage free space if we're satisfying
variable-sized requests. How do we minimze framentation? What are the
time and space overheads of alternate approaches?</p>
<p>Issues with free space:</p>
<ul>
<li>Fragmenting the memory</li>
</ul>
<p>Strategies to manage free space</p>
<ul>
<li>Best fit: search through the free list, find chunks of free memory
that are as big as the requested size. Then return one that is the
smalles of the group of candidates. "Best fit chunk". We can end up
paying for the exhaustive search.</li>
<li>Worst fit: find the largest chunk, return it, keep the remaining
large chunk on the free list</li>
<li>First fit: return the first big enough chunk</li>
<li>Next fit: keep a pointer to the location within the list where one
was looking last. Avoids exhaustive looking.</li>
</ul>
<p>Segregated lists: if an app has few popular-sized requests that it
makes. Keep a separate list just to manage objects of that size.
Anything else send to a more general allocator. An issue is how much
memory do you keep for this speciall allocator.</p>
<p>To fix this Jeff Bonwick invented the slab allocator. When the kernel
boots up, it allocates object caches for kernel objects requested
frequently. Thus these caches are segregated free lists of a given size
and serve quickly. When they're running low on space they ask for slabs
of memory from the general memory allocator.</p>
