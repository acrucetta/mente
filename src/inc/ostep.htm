<h1>Notes on Operating Systems in 3 Easy Pieces</h1>
<h3>Anki Questions</h3>
<h4>System Calls</h4>
<p><strong>What does wait() do?</strong> The parent process calls wait()
to delay its execution until the child finishes executing. When the
child is done, wait() returns to the parent.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> main<span class="op">()</span> <span class="op">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span>     <span class="dt">int</span> rc <span class="op">=</span> fork<span class="op">();</span> <span class="co">// Fork a new process</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>rc <span class="op">&lt;</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Fork failed; exit</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        fprintf<span class="op">(</span>stderr<span class="op">,</span> <span class="st">&quot;fork failed</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        exit<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="cf">if</span> <span class="op">(</span>rc <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Child process</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        printf<span class="op">(</span><span class="st">&quot;hello, I am child (pid:</span><span class="sc">%d</span><span class="st">)</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> <span class="op">(</span><span class="dt">int</span><span class="op">)</span>getpid<span class="op">());</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Parent process</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> wc <span class="op">=</span> wait<span class="op">(</span>NULL<span class="op">);</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        printf<span class="op">(</span><span class="st">&quot;hello, I am parent of </span><span class="sc">%d</span><span class="st"> (wc:</span><span class="sc">%d</span><span class="st">) (pid:</span><span class="sc">%d</span><span class="st">)</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> rc<span class="op">,</span> wc<span class="op">,</span> <span class="op">(</span><span class="dt">int</span><span class="op">)</span>getpid<span class="op">());</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>What does fork() do?</strong> The fork() system call is used
to create a new process</p>
<p><strong>What does exec() do?</strong> This system call is useful when
you want to run a program that is different from the calling program.
For example, calling fork() in p2.c is only useful if you want to keep
running copies of the same program</p>
<pre><code>
17 char *myargs[3];
18 myargs[0] = strdup(&quot;wc&quot;); // program: &quot;wc&quot; (word count)
19 myargs[1] = strdup(&quot;p3.c&quot;); // argument: file to count
20 myargs[2] = NULL; // marks end of array
21 execvp(myargs[0], myargs); // runs word count</code></pre>
<p>How does pipe() work? What are the main components? Why do we need
different modes of execution for a CPU? What is a trap, what is a trap
table? How can the OS effectively switch between processes? What do we
do when an OS process goes rogue? How does the CPU prevent it?</p>
<p><strong>What is a context switch?</strong></p>
<p>A context switch is conceptually simple: all the OS has to do is
<strong>save a few register values for the currently-executing
process</strong> (onto its kernel stack, for example) and
<strong>restore a few for the soon-to-be-executing process</strong>
(from its kernel stack).</p>
<p>By doing so, the OS thus <strong>ensures that when the
return-from-trap instruction is finally executed, instead of returning
to the process that was running, the system resumes execution of another
process.</strong></p>
<p><strong>How does the OS "baby proof" the CPU? What are the
equivalents?</strong></p>
<p>By during boot time setting up the trap handlers and starting an
interrupt timer, and then by only running processes in a restricted
mode. By doing so, the OS can feel quite assured that processes can run
efficiently, only requiring OS intervention to perform privileged
operations or when they have monopolized the CPU for too long and thus
need to be switched out.</p>
<p><strong>What is a timer interrupt? Why do we need it?</strong></p>
<p>The addition of a timer interrupt gives the OS the ability to run
again on a CPU even if processes act in a non-cooperative fashion. Thus,
this hardware feature is essential in helping the OS maintain control of
the machine.</p>
<h3>Scheduling</h3>
<p>What are the key assumptions made in the simple scheduling
examples?</p>
<ol type="1">
<li>Each job runs for the same amount of time.</li>
<li>All jobs arrive at the same time.</li>
<li>Once started, each job runs to completion.</li>
<li>All jobs only use the CPU (no I/O).</li>
<li>The run-time of each job is known.</li>
</ol>
<p>What are the key metrics for evaluating scheduling algorithms?</p>
<ul>
<li>Response time: time of first run - time of arrival</li>
<li>Turnaround time: time of completion - time of arrival</li>
</ul>
<p>What are the main types of simple scheduling policies mentioned and
what do they optimize for?</p>
<ul>
<li>First-in First-out (FIFO)</li>
<li>Shortest Job First (SJF): Optimizes turnaround time</li>
<li>Shortest Time-to-Completion First (STCF): Optimizes turnaround
time</li>
<li>Round Robin (RR): Optimizes response time</li>
</ul>
<p>What is the Multi-Level Feedback Queue (MLFQ) and what does it
optimize for?</p>
<p>MLFQ is a scheduling algorithm that varies job priority based on
observed behavior. It optimizes for turnaround time by running shorter
jobs first, while also trying to provide good responsiveness for
interactive users. It learns about processes as they run and uses their
history to predict future behavior.</p>
<p>What are the first two rules of the MLFQ?</p>
<ul>
<li>Rule 1: If Priority(A) &gt; Priority(B), A runs (B doesnâ€™t).</li>
<li>Rule 2: If Priority(A) = Priority(B), A &amp; B run in RR.</li>
</ul>
<p>How does MLFQ handle newly arriving jobs and jobs that use their
entire time slice?</p>
<ul>
<li>Rule 3: When a job enters the system, it is placed at the highest
priority (the topmost queue).</li>
<li>Rule 4a: If a job uses up an entire time slice while running, its
priority is reduced (i.e., it moves down one queue).</li>
</ul>
<h2>Lectures</h2>
<h3>Lecture 1</h3>
<p>CPUs give us the illusion of running a program.</p>
<p>CPUs give us the abstraction of a process. A running program.</p>
<p>Processes have a private memory and "address space"; they also have
registers, and a stack pointer.</p>
<p>The computer can run programs in Kernel Mode (OS) and User Mod (user
program). Limited number of things.</p>
<p>Special services to operate on the OS can be called system calls.</p>
<p>Boot Time:</p>
<ul>
<li>OS starts in Kernel mode; it tells the hardware where to jump to
when the user specifies a trap #</li>
<li>We set up trap handlers in the OS; tell H/W where the trap handlers
are in OS memory</li>
</ul>
<p>Key Questions</p>
<ul>
<li>What if OS wants to run a operation that's higher</li>
<li>What if the OS wants to stop A, then run B</li>
</ul>
<h3>Lecture 2</h3>
<p>A CPU is basically a while loop that fetches the program counter.
Figures out which instruction it is. And executes it.</p>
<pre><code>while (1) {
  fetch(pc)
  decode
  increment pc
  (before executing)
    check permission (kernel or os)
    if not ok raise exception
      OS can get involved
  execute (can change pg)
    process interrupts
}</code></pre>
<p>Before it executes; it checks permission. Is this instrunction OK to
execute.</p>
<ul>
<li>It basically checks permissions; if the instruction is OK to execute
do it. If not raise exception.</li>
</ul>
<p>A timer interrupt runs every once in a while. It makes the CPU decide
if it wants to run the current program or switch.</p>
<p>CPU virtualization mechanisms.</p>
<p>At boot time: OS runs first at priviledged mode (kernel mode).</p>
<ul>
<li>Install handlers (tell hardware what to run)</li>
<li>Tell H/W what code to run on exception/interrupt traps</li>
<li>Initialize timer interrupt</li>
<li>Ready ro run user programs</li>
</ul>
<p>Timeline:</p>
<ul>
<li>A trap instruction:
<ul>
<li>Transitions us from user mode to kernel mode</li>
<li>Jumps into OS: target trap handlers</li>
<li>Save register state (to execute later)</li>
<li>Return from trap (opposite of above)</li>
</ul></li>
</ul>
<p>The OS must track different user processes. It uses a <strong>process
list</strong></p>
<ul>
<li>Per-process info:
<ul>
<li>state: ready, running</li>
</ul></li>
</ul>
<h2>A big problem: some operations are slow; the OS needs to do what to
do.</h2>
<h2>Book Notes</h2>
<h3>Scheduling: Introduction</h3>
<p><strong>Key assumptions:</strong></p>
<ol type="1">
<li>Each job runs for the same amount of time.</li>
<li>All jobs arrive at the same time.</li>
<li>Once started, each job runs to completion.</li>
<li>All jobs only use the CPU (i.e., they perform no I/O) 5. The
run-time of each job is known.</li>
</ol>
<p>Types of scheduling:</p>
<ul>
<li><strong>First-in first out</strong></li>
<li><strong>Shortest job fist (SJF) - optimizes turnaround time</strong>
<ul>
<li>This new scheduling discipline is known as Shortest Job First (SJF),
and the name should be easy to remember because it describes the policy
quite completely: it runs the shortest job first, then the next
shortest, and so on.</li>
</ul></li>
<li><strong>Shortest time to completion first (STCF) - optimizes
turnaround time</strong>
<ul>
<li>Any time a new job enters the system, the STCF scheduler determines
which of the remaining jobs (including the new job) has the least time
left, and schedules that one. Thus, in our example, STCF would preempt A
and run B and C to completion</li>
</ul></li>
<li><strong>Round robin - optimizes response time</strong>
<ul>
<li>The basic idea is simple: instead of running jobs to completion, RR
runs a job for a time slice (sometimes called a scheduling quantum) and
then switches to the next job in the run queue.</li>
<li>The shorter the time slice the better; deciding on this length is a
trade-off</li>
<li>Any policy (such as RR) that is fair, i.e., that evenly divides the
CPU among active processes on a small time scale, will perform poorly on
metrics such as turnaround time</li>
</ul></li>
</ul>
<p><strong>Metrics</strong>:</p>
<ul>
<li>Response time: time of first run - time of arrival</li>
<li>Turnaround time: time of completion - time of arrival</li>
</ul>
<h3>Scheduling: Multi-Level Feedback Queue</h3>
<p>The Multi-level Feedback Queue (MLFQ) sched- uler was first described
by Corbato et al. in 1962 [C+62] in a system known as the Compatible
Time-Sharing System (CTSS), and this work, along with later work on
Multics, led the ACM to award Corbato its highest honor, the Turing
Award.</p>
<p>It optimizes for turnaround time. Done by running shorter jobs first.
It tries to make the system feel responsive to interactive users.
(minimizing staring at the screen and waiting)</p>
<p><em>Key Question: How do we schedule without perfect
knowledge?</em></p>
<p><strong>MLFQ</strong> varies the priority of a job based on its
observed behavior. If, for example, a job repeatedly relinquishes the
CPU while waiting for input from the keyboard, MLFQ will keep its
priority high, as this is how an interactive process might behave. If,
instead, a job uses the CPU intensively for long periods of time, MLFQ
will reduce its priority. In this way, MLFQ will try to learn about
processes as they run, and thus use the history of the job to predict
its future behavior.</p>
<p>Thus, we arrive at the first two basic rules for MLFQ: â€¢ Rule 1: If
Priority(A) &gt; Priority(B), A runs (B doesnâ€™t). â€¢ Rule 2: If
Priority(A) = Priority(B), A &amp; B run in RR.</p>
<p><strong>Changing Priorities</strong></p>
<ul>
<li>Rule 3: When a job enters the system, it is placed at the highest
priority (the topmost queue).</li>
<li>Rule 4a: If a job uses up an entire time slice while running, its
priority is reduced (i.e., it moves down one queue).</li>
<li>Rule4b: If it gives up the CPU before the time slice is up; stay at
same priority level</li>
</ul>
<p>Key issues:</p>
<ul>
<li>Starvation; what if we have a LOT of interactive jobs; long-running
CPU jobs will never finish</li>
<li>Gaming the scheduler: sneaky trick to get more time than its fair;
e.g., issue a quick IO before the time slice is up</li>
</ul>
<p><strong>Boosting priorities</strong></p>
<ul>
<li>Rule 5: After some time period S, move all the jobs in the system to
the topmost queue.</li>
</ul>
<p>It guarantees the processes won't starve by sitting in the top queue.
It also shares the CPU with other high priority jobs. If a job becomes
interactive it will receive a proper priority boost.</p>
<p><strong>Add Accounting</strong></p>
<ul>
<li>Rule 4: Once a job uses up its time allotment at a given level (re-
gardless of how many times it has given up the CPU), its priority is
reduced (i.e., it moves down one queue).</li>
</ul>
<p><em>MLFQ is interesting because instead of demanding a priori
knowledge of the nature of a job, it instead observes the execution of a
job and pri- oritizes it accordingly.</em></p>
<h3>Scheduling: Proportional Share</h3>
<p>In this case we want to optimize for each CPU process to obtain a
certain percentage of CPU time.</p>
<p>The key question is how can we degin a scheduler to share the CPU in
a proportional manner.</p>
<p>To solve for this, we can use tickets and randomness. We pull a
number from 0 to 99; assuming process A holds 0 through 74 and B 75
through 99. The winning ticket determies whether A or B runs.</p>
<p>It's not perfect but it will get us to a rough percentage over
time.</p>
<p>Ticket Mechanisms</p>
<ul>
<li>Ticket currency: allow users to allocate tickets among their own
jobs..</li>
<li>Ticket transfer: in client/server settings, allow the client to send
some tickets to the server</li>
<li>Ticket inflation: a process can temporarily raise or lower the
number of ticket it owns. If a process needs more CPU time it can raise
the ticket value to reflect that need to the system</li>
</ul>
<p>Implementation:</p>
<ul>
<li>We can use a linked list</li>
</ul>
<p>head -&gt; job A, tix 100 -&gt; job b, tix 50 -&gt; job c, tix 250
-&gt; null</p>
<p>Keep going over the list with a given tix number and once you go over
pick that job.</p>
<p>Deterministic Scheduling:</p>
<ul>
<li>We can also use "stride scheduling" which is basically representing
each job's stride as the inverse of the number of tickets they hold</li>
<li>At each point we pick the job with the lowest stride number. Then
increase its number. The jobs with the lowest stride (highest tickets)
will run more often</li>
<li>The downside is we have to maintain global state</li>
</ul>
<p>Proportional-share scheduleres are more useful in domains that are
easier to solve. E.g., a virtualized data center where you can assign
CPU cycles to given VMs.</p>
<h2>Virtualization</h2>
<p>Every address generated by a user program is a virtual address. The
OS gives an illusion that it has a large and private memory. It wants to
give programmers the illusion that you have a long contiguous address
space to put the code into data.</p>
<h3>Abstraction: Address Spaces</h3>
<p>Main Question:</p>
<ul>
<li>How can the OS build this abstraction of a private, potentially
large address space for multiple running processes (all sharing memory)
on top of a single, physical memory?</li>
</ul>
<p>Isolation is key in building reliable systems. The OS wants to run
programs independent of each other. If one fails, it wants to prevent
the other ones from failing. Some OS even wall off the OS from others
with microkernels. (compared with monolithic kernel designs)</p>
<p><strong>Goals of virtualizing memory:</strong></p>
<ul>
<li>Transparency - memory visible to the running program</li>
<li>Efficiency - fast processes using little memory; use hardware
support</li>
<li>Protection - protect processes from one another</li>
</ul>
<h3>Memory API</h3>
<p>Correct memory management has been such a problem, in fact, that many
newer languages have support for automatic memory manage- ment. In such
languages, while you call something akin to malloc() to allocate memory
(usually new or something similar to allocate a new object), you never
have to call something to free space; rather, a garbage collector runs
and figures out what memory you no longer have references to and frees
it for you.</p>
<p>Common issues:</p>
<ul>
<li>Forgetting to allocate memory</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>src <span class="op">=</span> <span class="st">&quot;hello&quot;</span><span class="op">;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>dst<span class="op">;</span> <span class="co">// oops! unallocated </span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>strcpy<span class="op">(</span>dst<span class="op">,</span> src<span class="op">);</span> <span class="co">// segfault and dieA</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>src <span class="op">=</span> <span class="st">&quot;hello&quot;</span><span class="op">;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>dst <span class="op">=</span> <span class="op">(</span><span class="dt">char</span> <span class="op">*)</span> malloc<span class="op">(</span>strlen<span class="op">(</span>src<span class="op">)</span> <span class="op">+</span> <span class="dv">1</span><span class="op">);</span> </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>strcpy<span class="op">(</span>dst<span class="op">,</span> src<span class="op">);</span> <span class="co">// work properly</span></span></code></pre></div>
<ul>
<li>Not allocating enough memory</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>src <span class="op">=</span> <span class="st">&quot;hello&quot;</span><span class="op">;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>dst <span class="op">=</span> <span class="op">(</span><span class="dt">char</span> <span class="op">*)</span> malloc<span class="op">(</span>strlen<span class="op">(</span>src<span class="op">));</span> <span class="co">// too small! </span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>strcpy<span class="op">(</span>dst<span class="op">,</span> src<span class="op">);</span> <span class="co">// work properly</span></span></code></pre></div>
<ul>
<li>Forgetting to initialize allocated memory</li>
<li>Forgetting to free memory</li>
<li>Freeing memory before you're done aka dangling pointers</li>
<li>Double freeing memory</li>
</ul>
<p>Good tools to prevent issues: check out both purify [HJ92] and
valgrind [SN05]; both are excellent at helping you locate the source of
your memory-related problems.</p>
<h2>Project 1B</h2>
<p>What are system calls?</p>
<ul>
<li>A call between you and the machine, you call the OS and ask him to
do stuff for you</li>
</ul>
<h3>Virtual Memory: Address Translation</h3>
<p>Address translation implies mapping the virtual addresses to the
physical space. This happens in the kernel space since its a priviledged
operation. If we don't find an address we might get a segmentation
fault.</p>
<p>The most popular method is dynamic relocation which is:</p>
<p><code>physical address = virtual address + base</code></p>
<p>The hardware provides base and bound registers. The hardware needs to
check whether the address is valid. This is done by the base and bounds
register.</p>
<p>The OS also needs to know which areas of memory are free through the
"free list".</p>
<pre><code>ARG seed 0
ARG address space size 1k
ARG phys mem size 64k

Base-and-Bounds register information:

  Base   : 0x00008000 (decimal 32768)
  Limit  : 16384

Virtual Address Trace
  VA  0: 0x00000360 (decimal:  864) --&gt; VALID: 0x00008360 (decimal: 33632)
  VA  1: 0x00000308 (decimal:  776) --&gt; VALID: 0x00008308 (decimal: 33544)
  VA  2: 0x000001ae (decimal:  430) --&gt; VALID: 0x000081ae (decimal: 33198)
  VA  3: 0x00000109 (decimal:  265) --&gt; VALID: 0x00008109 (decimal: 33033)
  VA  4: 0x0000020b (decimal:  523) --&gt; VALID: 0x0000820b (decimal: 33291)</code></pre>
<h2>Segmentation</h2>
<p>Key question: how do we support a large address space with many gaps
between the stack and the heap.</p>
<p>If we segment, we can place each one of the segments in diff parts of
the physical memory avoiding unused virtual address space.</p>
<p>The infamous term segmentation fault arises from a memory access on a
segmented machine to an illegal address.</p>
<p>We can have coarse grained and fine-grained segmentation.</p>
<p>There are many algorithms to minimize external framgentation. Which
means there's not one perfect solution.</p>
<p>There are issues when we try allocating variable-sized segments in
memory. The main one is external fragmentation. Because they get chopped
up it can be hard to allocate the right amount of memory. We also still
may have large segments of unused memory. The heap still needs the full
allotted space to work.</p>
<pre><code>ARG seed 2
ARG address space size 128
ARG phys mem size 1024k

Segment register information:

  Segment 0 base  (grows positive) : 0x00000000 (decimal 0)
  Segment 0 limit                  : 20

  Segment 1 base  (grows negative) : 0x00000200 (decimal 512)
  Segment 1 limit                  : 20

Virtual Address Trace
  VA  0: 0x0000007a (decimal:  122) --&gt; VALID in SEG1: 0x000001fa (decimal:  506)
  VA  1: 0x00000079 (decimal:  121) --&gt; VALID in SEG1: 0x000001f9 (decimal:  505)
  VA  2: 0x00000007 (decimal:    7) --&gt; VALID in SEG0: 0x00000007 (decimal:    7)
  VA  3: 0x0000000a (decimal:   10) --&gt; VALID in SEG0: 0x0000000a (decimal:   10)
  VA  4: 0x0000006a (decimal:  106) --&gt; SEGMENTATION VIOLATION (SEG1)</code></pre>
<p>In the sample address space above. We have 128 bye address spaces
mapped two two segments. Segment 0 is the heap and segment 1 the stack
(grows up). We can only map values it seems that grow from 0 to 20 or
from 128 to 108. Therefore the valid addresses are 0-20 and 108-128. 106
causes a segment violation and it can't be mapped to our memory.</p>
<h3>Free-Space Management</h3>
<p>Key question: how do we manage free space if we're satisfying
variable-sized requests. How do we minimze framentation? What are the
time and space overheads of alternate approaches?</p>
<p>Issues with free space:</p>
<ul>
<li>Fragmenting the memory</li>
</ul>
<p>Strategies to manage free space</p>
<ul>
<li>Best fit: search through the free list, find chunks of free memory
that are as big as the requested size. Then return one that is the
smalles of the group of candidates. "Best fit chunk". We can end up
paying for the exhaustive search.</li>
<li>Worst fit: find the largest chunk, return it, keep the remaining
large chunk on the free list</li>
<li>First fit: return the first big enough chunk</li>
<li>Next fit: keep a pointer to the location within the list where one
was looking last. Avoids exhaustive looking.</li>
</ul>
<p><strong>Segregated lists</strong></p>
<p>If an app has few popular-sized requests that it makes. Keep a
separate list just to manage objects of that size. Anything else send to
a more general allocator. An issue is how much memory do you keep for
this speciall allocator.</p>
<p>To fix this Jeff Bonwick invented the slab allocator. When the kernel
boots up, it allocates object caches for kernel objects requested
frequently. Thus these caches are segregated free lists of a given size
and serve quickly. When they're running low on space they ask for slabs
of memory from the general memory allocator.</p>
<p><strong>Buddy allocation</strong></p>
<p>Memory is thought of as a big space of size 2^N. When the request is
made. THe search divides free space by two until a block that is big
enough can be returned. Then its returned to the user.</p>
<p>When the block is freed. The allocator checks whether the buddy 8KB
is free. If so, it coalesces the blocks. And so on and so forth. It
stops when a buddy is found in use.</p>
<p>There are many trade-offs in these allocators. The more you know
about the workloads, the more you can tune it to work better for that
workload. Making it fast and spece-efficient is still a big
challenge.</p>
<p>In the exercise, we can see how changing the order of the sizes can
speed up searching through the lists. When we use worst fit, and sort in
descending order, the searching is quick. If it was sorted in ascending
order it would take close to O(N).</p>
<p>When we coalesce the list, even if we have a lot of ops, we end up
with a smaller free-list. I assume this would lower the latency of the
process.</p>
<p>When many of the operations were to free vs. allocate we returned
more and less failures in allocating memory. It makes sense, when we run
out of memory we start returning -1 to the malloc() functions.</p>
<h3>Paging: Introduction</h3>
<p>Key question: how do we virtualize memory with pages to avoid the
problems of segmentation.</p>
<h3>The Page Table</h3>
<p>One of the most important data structures in the memory management
subsystem of a modern OS is the page table.</p>
<p>It stores virtual-to-physical address translations. Letting the
system know where each page of an address resides in physical
memory.</p>
<p>Structure of a page table:</p>
<p>PFN | G | PAT | D | A | PCD | PWT | U/S | R/W | P</p>
<ul>
<li>We have a present bit</li>
<li>A reference bit</li>
<li>A read/write bit</li>
<li>A user/supervisor bit</li>
<li>Hardware caching bits</li>
<li>An accessed bit</li>
<li>A dirty bit</li>
<li>And the page-frame-number itself</li>
</ul>
<p><strong>Formula to calculate map from virtual to phyisical
memory</strong></p>
<p>The main page table operation can be exemplified by:</p>
<p><code>movl 21, %eax</code></p>
<p>This means that we're referencing address 21. We want to translate
virtual address 21 into the proper address 117. To do so we need to run
the instructions below to find the physical address.</p>
<p>To find the location of the desired page table entry.</p>
<pre><code>// Extract the VPN from the virtual address
VPN = (virtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT

// Form the address of the PTE
PTEAddr = PageTableBase Register + (VPN * sizeof(PTE))</code></pre>
<p>In the example above shift is 4 (number of bits in the1 offset).
Assuming we have a virtual address 21 (010101); the masking turns this
value into 010000; the shift turns it into 01 which would be the virtual
page 1.</p>
<p>We then use this value to index the array of PTEs.</p>
<pre><code>// Access is OK, form the physical address and fetch it
offset = virtual address &amp; offset mask
physical address = (PFN &lt;&lt; shift) | offset</code></pre>
<p>With this physical address, the hardware can fetch the desired data
from memory and put it into register eax.</p>
<p>Summary code:</p>
<pre><code>// Extract the VPN from the virtual address
VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT

// Form the address of the page-table entry (PTE)
PTEAddr = PTBR + (VPN * sizeof(PTE))

// Fetch the PTE
PTE = AccessMemory(PTEAddr)

// Check if process can access the page
if (PTE.Valid == False)
    RaiseException(SEGMENTATION_FAULT)
else if (CanAccess(PTE.ProtectBits) == False)
    RaiseException(PROTECTION_FAULT)
else
// Access is OK: form physical address and fetch it
offset = VirtualAddress &amp; OFFSET_MASK
PhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offset
Register = AccessMemory(PhysAddr)</code></pre>
<p><strong>Example translation</strong></p>
<p>Virtual Address 4: 0x00003a1e (Decimal: 14878)</p>
<p><strong>Extract VPN and Offset:</strong></p>
<ul>
<li>Hexadecimal 0x3a1e to binary is 11 1010 0001 1110.</li>
<li>VPN: Top 2 bits â†’ 11 (binary) â†’ 3 (decimal).</li>
<li>Offset: Lower 12 bits â†’ 1010 0001 1110 â†’ 0xa1e (hex).</li>
</ul>
<p><strong>Check Page Table for VPN 3:</strong></p>
<ul>
<li>Entry: 0x80000006 (Valid, PFN = 6).</li>
</ul>
<p><strong>Calculate Physical Address:</strong></p>
<ul>
<li>PFN 6 to base address: 6 * 4096 = 24576 or 0x6000.</li>
<li>Physical Address: 0x6000 + 0xa1e = 0x6a1e.</li>
</ul>
<h3>Reflection questions</h3>
<p>What happens as address space grows?</p>
<ul>
<li>We get more potential page tables</li>
</ul>
<p>What happens as we increase page size?</p>
<ul>
<li>We get less page tables; bigger ones</li>
</ul>
<p>Why not use big page sizes in general?</p>
<ul>
<li>As we increase the size of the pages, we get less pages in the page
table entry.</li>
<li>The bigger the tables the more fragmentation.</li>
<li>Bigger pages mean smaller processes must still be allocated a full
large page.</li>
<li>Larger pages can reduce the effectiveness of cache systems. Ihey can
decrease the locality of reference.</li>
<li>When page fault occur, the system must load the required page from
disk. Larger pages means more transferred data per page fault, which can
slow down the response time</li>
</ul>
<p>What happens as you increase the percentage of pages that are
allocated in each address space?</p>
<ul>
<li>We suffer more page faults because the physical memory is busy</li>
<li>The system might have to swap pages in and out more frequently</li>
</ul>
<h3>Paging: Faster Translations</h3>
<p>Key question: how can we speed up address translation, and generally
avoid the extra memory reference that paging seems to require? What kind
of hardware support do we need?</p>
<p>To speed up translations we use a cache. We call it the
translation-lookaside buffer (TLB). It is simply a hardware cache of
popular translations. A better name would be an address-translation
cache.</p>
<p>Example algorithm:</p>
<pre><code>VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT
(Success, TlbEntry) = TLB_Lookup(VPN)
if (Success == True)   // TLB Hit
    if (CanAccess(TlbEntry.ProtectBits) == True)
        Offset   = VirtualAddress &amp; OFFSET_MASK
        PhysAddr = (TlbEntry.PFN &lt;&lt; SHIFT) | Offset
        Register = AccessMemory(PhysAddr)
    else
        RaiseException(PROTECTION_FAULT)
else                  // TLB Miss
    PTEAddr = PTBR + (VPN * sizeof(PTE))
    PTE = AccessMemory(PTEAddr)
    if (PTE.Valid == False)
        RaiseException(SEGMENTATION_FAULT)
    else if (CanAccess(PTE.ProtectBits) == False)
        RaiseException(PROTECTION_FAULT)
    else
        TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
        RetryInstruction()</code></pre>
<p><strong>Caching</strong></p>
<p>Caching is an important principle of this process. It relies on
spatial locality and temporal locality. Spatial locality means if a
program accesses memory in an area X, it might access areas nearby in
the future. Temporal locality means if an instruction is commonly
accessed, it might be re-accessed again in the future.</p>
<p>All of this relies on the cache being small. Once we try to grow it
too much we will have the same issues of accessing other types of
memory.</p>
<p><strong>Who handles TLB misses?</strong></p>
<p>Hardware and software people didn't trust each other. Thus the
hardware sometimes would handle the TLB miss entirely.</p>
<p>Most architectures now (RISC) handle the TLB with a software-managed
TLB. On a TLB miss, the hardware raises an exception, which pauses the
instruction stream, raises the priviledge to kernel mode and jumps to a
trap handler.</p>
<p><strong>RISC vs. CISC</strong></p>
<p>There used to be two camps. Complex Instruction Set Comptuing (CISC)
and Reduced ISC.</p>
<p>CISC sets had a lot of instructions in them; each one was really
powerful. The idea was that instructions should be high-level
primitives, to make the assembly language easier to use.</p>
<p>RISC are the opposite. They argued to rip out as much hardware as
possible and make whats left really fast. RISC chips were faster and
MIPS and Sun were started out of it.</p>
<p>Intel then started making their CISC chips faster and now we have
both combinations.</p>
<p><strong>TLB Contents</strong></p>
<p>TLBs might have 32, 64, or 128 entires.</p>
<p>VPN | PFN | other bits</p>
<p>Key question: how do we handle the TLB during context switches?</p>
<p>A potential solution is to flush it every time. But this will be
costly because then we need to incur new TLB misses until it is
refreshed.</p>
<p>Another solution can be adding an identifier to the process that's
using each entry. This is known as the Address Space Identifier (ASID).
Similar to a process identifier but with fewer bits.</p>
<p><strong>Issue: Replacement Policies</strong></p>
<p>What happens when we need to replace a TLB entry? we can use the
Least Recently Used method or a random method. Each one has its pros and
cons. The LRU can behave unreasonably in certain occassions; so the
random one might avoid edge cases in other times.</p>
<p><strong>Reflection questions:</strong></p>
<ul>
<li>Why do we use TLBs when managing address translations?</li>
<li>What happens if we didn't have TLBs?</li>
<li>How do we deal with TLBs across different processes?</li>
<li>What's the max amount of entries and mappings a TLB could
handle?</li>
</ul>
<h2>Paging: Smaller Tables</h2>
<p>Key question: how to make page tables smaller? Linear page tables are
too big; taking up far too much memory on typical systems. How can we
make them smaller?</p>
<p>If we make them too big, we have internal fragmentation. Often we
just end up using 4KB and 8KB pages.</p>
<p>E.g., If we have a page table for a 16KB address space with 1KB
pages; we might have 3-4 used pages but the other ones might be
empty.</p>
<h3>Multi-Level Page Table</h3>
<p>We use a page directory that points to each page table. Each entry in
this page diretory has a valid bit and a page frame number. The valid
bit indicates if any of the entries in the page table it points to is
valid.</p>
<pre><code>PDE (Page Directory Entries)
  Valid Bit -&gt; Tell us if the page table has any valid entries
  Page Frame Number -&gt; Points to the page table

Page Table
  Valid Bit
  Protection
  Page Frame Number

Page frame (X bits of info)
  Contains the a set of bits with information on the process

Page-Directory Entry:
PDEAddr = PageDirBase + (PDIndex + sizeof(PDE))
PTEAddr = (PDE.PFN &lt;&lt; SHIFT) + (PTIndex * sizeof(PTE))</code></pre>
<h3>Tip</h3>
<p>Be wary of design trade-offs. Always implement the least complex
system that achieves the task at hand. Avoid needless complexity, in
prematurely-optimized code or other forms. It makes it harder to
understand, maintain and debug the code.</p>
<p>Reflection question:</p>
<ul>
<li>Why do we use page tables in the first place: they're used to
translate virtual to physical addresses. If the page table is too small
we might have collissions?</li>
<li>How do address translations work in multi-level page entries</li>
<li>What are the time-space trade-offs of linear page tables vs.
multi-level page tables</li>
</ul>
<h2>Beyond Physical Memory: Mechanisms</h2>
<p>General page mechanism:</p>
<ul>
<li>The hardware extracts the VPN (virtual page number) from the virtual
address</li>
<li>It then checks the TLB (dictionary cache) for a match; if its a hit,
it return the physical address</li>
<li>If not, it looks up the page table in memory (using the page table
base register) and looks up the entry (page table entry) using the VPN
as the index</li>
<li>If the table is valid and present, we extract the page frame number,
add it to the cache, and retry generating a cache hit next time</li>
</ul>
<p>What if we try to access the page but its not in memory? It is
normally determined through a "present bit". If its set to 1, we're
good. If not (its on the disk) then we we cause a <strong>page
fault</strong></p>
<p>The page fault is handled by a <strong>page fault handler</strong>
which is in the software.</p>
<p>If the page is not present, and has been swapped to disk. The OS is
going to <em>swap</em> the page into memory to service the page fault.
It will use the PTE to find the addrtess and issue the request to the
disk.</p>
<p><strong>What if the OS is full?</strong></p>
<p>We then need to replace the page. Here comes the page replacement
policy.</p>
<pre><code> PFN = FindFreePhysicalPage()
// no free page found
2 if (PFN == -1)
    // run replacement algorithm
    3 PFN = EvictPage()
// sleep (waiting for I/O)
4 DiskRead(PTE.DiskAddr, PFN)
// update page table with present
5 PTE.present = True
// bit and translation (PFN)
6 PTE.PFN = PFN
7 RetryInstruction()</code></pre>
<p>To keep the memory free the OS has a low and high watermark. It
proactively evicts tables until we have a certain amount of free pages.
This is done by the page daemon.</p>
<p>Key questions:</p>
<ul>
<li>What type of storage devices are easier or faster to swap
spaces?</li>
<li>What happens when a program needs to access memory?</li>
</ul>
<h2>Beyond Physical Memory: Policies</h2>
<p><strong>Key question</strong>: what page do we evict when we run out
of memory?</p>
<p>We want to maximize the amount of cache hits. Number of times a page
can be accessed in memory.</p>
<p>We use the "Average Memory Access Time"</p>
<p>$AMAT = T_M + P_{Miss} * T_D$</p>
<p>TM represents the cost of accessing memory, TD the cost of ac-
cessing disk, and PMiss the probability of not finding the data in the
cache (a miss); PMiss varies from 0.0 to 1.0,</p>
<p><strong>What is the optimal policy?</strong></p>
<p>Replace the page that will be accessed the furthest in the future.
Resulting in the fewest cache misses.</p>
<p>There are 3 types of cache misses (Three Cs)</p>
<ul>
<li><strong>Compulsory miss aka cold start miss</strong>: it occurs
because it was empty to being with and you need to add it</li>
<li><strong>Capacity miss</strong>: the cache ran out of space and had
to evict an item to bring a new one to the cache</li>
<li><strong>Conflict miss</strong>: no space available. Doesn't have in
OS page cache because there's no restrictions where can put a page.</li>
</ul>
<p><strong>Simpler Policy: FIFO</strong></p>
<p>We evict the caches in order. It doesn't perform as well as the
optimal more complicated policy. Pages are placed in a queue when they
enter the system. When we need to replace we evict the page at the end
of the queue.</p>
<p><strong>Another Simple Policy: Random</strong></p>
<p>Simply picks a random page to replace under memory pressure. Can do
better or worst. But simple to implement.</p>
<p><strong>Usage History Policy: LRU</strong></p>
<p>A more commonly- used property of a page is its
<strong>recency</strong> of access; the more recently a page has been
accessed, perhaps the more likely it will be accessed again.</p>
<p>This is based on the "principle of locality". Programs tend to access
certain code and data structures frequently. We should use history to
figure out which pages are important and keep them.</p>
<p>We then have Least Frequently Used and Least Recently Used.</p>
<p><strong>Types of Locality</strong></p>
<ul>
<li>Spatial: pages around a given page will be accessed more
frequently</li>
<li>Temporal: pages accessed often will be accessed again</li>
</ul>
<p>The performance of each policy will vary based on the workload.</p>
<ul>
<li>Random workload: they all perform about the same.</li>
<li>80/20 workload (some memories get accessed more often): LRU and the
Optimal perform better than FIFO and Random</li>
<li>Looping workload (for loop): OPT and LRU perform better but random
performs not too bad.</li>
</ul>
<h2>Virtualization Key Concepts</h2>
<ul>
<li>What is a system call and how does it work?</li>
<li>What is a context switch?</li>
<li>What are some common process scheduling algorithms?</li>
<li>How does a process reserve memory?</li>
<li>What are the different ways in which memory can be stored?
(Segmentation and paging)</li>
<li>How does segmentation work?</li>
<li>What is the free-space management problem in memory allocation?</li>
<li>What is the principle of locality in caching?</li>
<li>How does paging work?</li>
<li>What are page replacement algorithms and why are they
necessary?</li>
<li>How does the system map virtual memory to physical memory?</li>
<li>What happens when the OS runs out of memory?</li>
<li>What happens if we have huge page tables? What are the drawbacks?
<ul>
<li>e.g., Internal fragmentation</li>
</ul></li>
<li>How do page tables work?</li>
<li>What are the different types of page tables?</li>
<li>How are multi-level page tables better than linear ones?</li>
<li>What is a TLB? When is it used?</li>
<li>What happens during TLB misses?</li>
<li>What are some policies we use to replace cache in TLBs?</li>
<li>How do segmentation fault occurs?</li>
</ul>
<h1>Phase 2: Concurrency</h1>
<h2>Introduction to Concurrency</h2>
<p>Concurrency is great because it allows us to do parallel programming
and to run the program while we wait for I/O operations.</p>
<p>Concurrency is harder than single-threaded programs because we now
have to manage shared data and state across threads.</p>
<p>We can run into issues such as data races and deadlocking among
others.</p>
<p>To solve for this, Dijkstra invented atomic operations and sync
primitives that allow us to give some guarantees to the program.</p>
<p>An example is sempahores. They guarantee at most N threads will pass
through a critical section.</p>
<p>Another example is incrementing a counter with two threads, if we
don't use sync primitives we may run into an indeterministic program.
i.e., run it twice and get different results.</p>
<p>The most important questions are:</p>
<ul>
<li>What support do we need from the hardware in order to build useful
synchronization primitives?</li>
<li>What support do we need from the OS?</li>
<li>How can we build these primitives correctly and efficiently?</li>
<li>How can programs use them to get the desired results?</li>
</ul>
<p>The key terms are:</p>
<ul>
<li>A critical section: two threads accessing a shared resource</li>
<li>A race condition: two threads targeting a critical section</li>
<li>An indeterminate program vs. indeterminate: random vs. not</li>
<li>Mutual exclusion primitives: 1 single thread ever entering critical
sections</li>
</ul>
<p>An useful tool to validate threads in C is helgrind, run it with
<code>valgrind --tool=helgrind ./main-signal-cv</code></p>
<h2>Thread APIs</h2>
<ul>
<li>pthread_create()</li>
<li>pthread_join()</li>
<li>pthread_mutex_lock(&amp;lock);</li>
<li>pthread_mutex_unlock(&amp;lock);</li>
<li>pthread_cond(&amp;lock);</li>
</ul>
<h2>Locks</h2>
<p>Key Question:</p>
<ul>
<li>How do we build an efficient lock? It should provide mutual
exclusion at a low cost</li>
</ul>
<p>A good lock should provide:</p>
<ul>
<li>Fairness</li>
<li>Mutual exclusion</li>
<li>No starvation</li>
<li>Performance (as low overhead as possible)</li>
</ul>
<p>Locks can have hardware support and OS support to run properly. To
build a good lock we need to assume a little hardware support. That way
we can meet all the properties above.</p>
<h3>Spin lock using Test-and-set</h3>
<p>As long as the lock is held by another thread, TestAndSet() will
repeatedly return 1, and thus this thread will spin and spin until the
lock is finally released.</p>
<pre><code>typedef struct __lock_t {
    int flag;

void init(lock_t *lock) {
    // 0: lock is available, 1: lock is held
    lock-&gt;flag = 0;
}

void lock(lock_t *lock) {
        // It tests the old value while simultaneously sets it to
        // a new value
    while (TestAndSet(&amp;lock-&gt;flag, 1) == 1)
        ; // spin-wait (do nothing)
}
void unlock(lock_t *lock) {
    lock-&gt;flag = 0;
}</code></pre>
<p>Questions:</p>
<ul>
<li>How do we measure the CPU impact of different types of locks?</li>
</ul>
<h3>Compare-And-Swap</h3>
<p>Checks if the value at an address is expected, if so updates it with
a new value.</p>
<pre><code>   void lock(lock_t *lock) {
          while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)
                    ; // spin</code></pre>
<h3>Load-Linked and Store-Conditional</h3>
<ul>
<li><strong>Load-Linked</strong>: fetches a value from memory and places
it in a register</li>
<li><strong>Store-Conditional</strong>: only succeeds if no intervening
store to the address has taken place</li>
</ul>
<p>Potential lock:</p>
<ul>
<li>We fetch a value from memory</li>
<li>Only succeed if no store has taken place</li>
</ul>
<p>If there's not a value stored in memory keep rotating.</p>
<p>E.g.,</p>
<pre><code>int LoadLinked(int *ptr) {
    return *ptr;
}

int StoreConditional(int *ptr, int value) {
    if (no update to *ptr since LoadLinked to this address) {
        *ptr = value;
        return 1; // success!
    } else {
        return 0; // failed to update
    }
}

void lock(lock_t *lock) {
       while (1) {
           while (LoadLinked(&amp;lock-&gt;flag) == 1)
               ; // spin until itâ€™s zero
           if (StoreConditional(&amp;lock-&gt;flag, 1) == 1)
               // if set-it-to-1 was a success: all done
               // otherwise: try it all over again
               return; 
   
   void unlock(lock_t *lock) {
       lock-&gt;flag = 0;
    }</code></pre>
<h3>Fetch-And-Add</h3>
<pre><code>int FetchAndAdd(int *ptr) {
    int old = *ptr;
    *ptr = old + 1;
    return old;
}

typedef struct __lock_t {
    int ticket;
    int turn;
} lock_t;


void lock_init(lock_t *lock) {
    lock-&gt;ticket = 0;
    lock-&gt;turn = 0;
}

void lock(lock_t *lock) {
    int myturn = FetchAndAdd(&amp;lock-&gt;ticket);
    while (lock-&gt;turn != myturn)
        ; // spin
}

void unlock(lock_t *lock) {
    lock-&gt;turn = lock-&gt;turn + 1;
}</code></pre>
<h3>OS Support</h3>
<p>In the case of some locks, the OS can allows us to yield or park
certain threads. This works well in some cases.</p>
<p>Yield lets us yield the CPU to other threads while our current thread
is blocked.</p>
<p>Park allows us to put a calling thread to sleep and unpack to wake it
up by its <code>threadID</code></p>
