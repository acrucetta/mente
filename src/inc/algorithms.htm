<h2 id="notes-on-algorithms">Notes on Algorithms</h2>
<p><em>Algorithms: method for solving a problem.</em></p>
<p><em>Data structure: method to store information.</em></p>
<p><em>Algorithms + Data Structures = Programs</em></p>
<h2 id="week-1-quick-union">Week 1: Quick Union</h2>
<p>Steps to develop a usable algorithm:</p>
<ol type="1">
<li>Model the problem</li>
<li>Find an algorithm to solve it</li>
<li>Fast enought / fits in memory?</li>
<li>If not, figure out why</li>
<li>Find a way to address the problem</li>
<li>Iterate until satisfied</li>
</ol>
<h3 id="dynamic-connectivity">Dynamic Connectivity</h3>
<p>Is there a path between two objects? Used in many applications. The
union-find is a problem of maintaining a collection of disjoint sets and
performing two operations.</p>
<p>We need to implement: find query and union command.</p>
<p>Find query: check if two objects are in the same component. Union:
replace components with their union.</p>
<p>We need to check our API design before implementing it.</p>
<p><strong>Quick Find (eager approach)</strong>:</p>
<ul>
<li>Data structure: integer array id[] of size N</li>
<li>Interpretation: two objects are connected if they have the same
ID.</li>
</ul>
<p>Cost model: numer of array accesses.</p>
<p>Find: check if p and q have the same id.</p>
<p>Union: to merge components containing p and q, change all entries
whose id equals id[p] to id[q].</p>
<p><strong>Quick Union (lazy approach):</strong></p>
<ul>
<li>Data structure: integer array id[] of size N</li>
<li>Interpretation: id[i] is parent of i</li>
<li>Root of i is id[id[id[…id[i]…]]]</li>
</ul>
<p>Find: check if p and q have the same root.</p>
<p>Union: to merge components containing p and q, set the id of p’s root
to the id of q’s root.</p>
<p>Quick-find: union too expensive. Trees are flat.</p>
<p>Quick-union: trees can get tall. Find too expensive (could be N array
accesses).</p>
<h3 id="improvements">Improvements</h3>
<p>Weighted quick-union</p>
<ul>
<li>Modify quick-union to avoid tall trees</li>
<li>Keep track of size of each tree (number of objects)</li>
<li>Balance by linking root of smaller tree to root of larger tree</li>
</ul>
<p>In sumamry, we try to avoid tall trees as we iterate through the
array.</p>
<figure>
<img src="image-1.png" alt="weighted tree comparison" />
<figcaption aria-hidden="true">weighted tree comparison</figcaption>
</figure>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuickUnion:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">id</span> <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sz <span class="op">=</span> [<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> root(<span class="va">self</span>, i):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> i <span class="op">!=</span> <span class="va">self</span>.<span class="bu">id</span>[i]:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            i <span class="op">=</span> <span class="va">self</span>.<span class="bu">id</span>[i]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> i</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> connected(<span class="va">self</span>, p, q):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.root(p) <span class="op">==</span> <span class="va">self</span>.root(q)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> union(<span class="va">self</span>, p, q):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> <span class="va">self</span>.root(p)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> <span class="va">self</span>.root(q)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">id</span>[i] <span class="op">=</span> j</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> weighted_union(<span class="va">self</span>, p, q):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> <span class="va">self</span>.root(p)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> <span class="va">self</span>.root(q)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> j:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.sz[i] <span class="op">&lt;</span> <span class="va">self</span>.sz[j]:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">id</span>[i] <span class="op">=</span> j</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sz[j] <span class="op">+=</span> <span class="va">self</span>.sz[i]</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">id</span>[j] <span class="op">=</span> i</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sz[i] <span class="op">+=</span> <span class="va">self</span>.sz[j]</span></code></pre></div>
<p><strong>Running time:</strong></p>
<ul>
<li>Find: takes time proportional to depth of p and q</li>
<li>Union: takes constant time, given roots</li>
</ul>
<p>Depth of any node x is at most lg N.</p>
<h3 id="union-find-applications">Union-Find Applications</h3>
<ul>
<li>Games</li>
<li>Dynamic connectivity</li>
<li>Percolation</li>
</ul>
<p><strong>Percolation</strong></p>
<p>N by N grid sites. A system percolates iff top and botom are
connected by open sites.</p>
<p>Can be thought of as water flowing through surfaces. Or in Social
networks if we want to know whether people are connected.</p>
<p><strong>The subtext of the problem is:</strong></p>
<ul>
<li>We model the problem</li>
<li>Then we find an algorithm</li>
<li>We check whether it’s fast or not</li>
<li>We address the problem</li>
<li>And iterate…</li>
</ul>
<h2 id="week-2-analysis-of-algorithms">Week 2: Analysis of
Algorithms</h2>
<p>The key is running time; we used to have a cranking machine; how many
cranks we need to do to compute.</p>
<p>Why analyze algorithms?</p>
<ul>
<li>Predict performance</li>
<li>Compare algorithms</li>
<li>Provide guarantees</li>
<li>Understand theoretical basis</li>
</ul>
<p>One of the most important ones is the FFT algorithm; takes only <span
class="math inline"><em>N</em><em>l</em><em>o</em><em>g</em><em>N</em></span>
steps. Another one is the N-body simulation.</p>
<p>We use the scientific method to analyze algorithms: Observe,
hypothesize, predict, verify, and validate.</p>
<p>Experiments must be <strong>reproducible</strong> and
<strong>falsifiable</strong>.</p>
<p><strong>3-Sum Example</strong></p>
<p>How many distinct integers add up to zero.</p>
<p>Brute force: do a triple for loop. (it will take n^3)</p>
<p><strong>Mathematical models of runtime</strong></p>
<p>Donald Knuth first proposed the total run-time when programs were
running for too long.</p>
<p>E.g., how many instructions as a function of input size N?</p>
<p>Turing said we should just count the most expensive operations
instead of each addition.</p>
<p>By focusing on one operation you can simplify the frequency
counts.</p>
<p><strong>Order of growth classifications</strong></p>
<p>Small set of functions: log N, N, NlogN, N^2, N^3, 2^N</p>
<figure>
<img src="image-3.png" alt="Alt text" />
<figcaption aria-hidden="true">Alt text</figcaption>
</figure>
<p>Based on binary search we can find a better algorithm for 3-Sum. We
can use N^2 log N instead of N^3.</p>
<p>Instructions:</p>
<ul>
<li>Sort the N integers
<ul>
<li>Insertion sort: N^2</li>
</ul></li>
<li>For each pair of integers a and b, binary search for -(a+b)
<ul>
<li>Binary search: log N</li>
</ul></li>
<li>Only count if a[i] &lt; a[j]] &lt; a[k]</li>
</ul>
<h3 id="types-of-analysis">Types of analysis</h3>
<p>We have best case, worst case, and average case. Lower bound on cost,
upper bound on cost, and expected cost.</p>
<p>We can have different approaches: 1. Design for worst case 2.
Randomize the input</p>
<p>The main approach is to reduce variability by focusing on the worst
case scenario. We want to find an optimal algorithm.</p>
<p>We have many notations - Big Theta (<span
class="math inline"><em>B</em><em>i</em><em>g</em><em>θ</em></span>):
asymptotic order of growth - Big Oh: to develop upper bounds - Big
Omega: to develop lower bounds</p>
<p>Example: - 3 Sum - Improved algorithm gives us O(<span
class="math inline"><em>N</em><sup>2</sup><em>l</em><em>o</em><em>g</em><em>N</em></span>)
- Lower bound (proof that no algorithm can do better): <span
class="math inline"><em>ω</em>(<em>N</em>)</span></p>
<p>The approach: - Develop algorithm - Prove a lower bound</p>
<p>We can also have tilde notation. It’s used to provide an approximate
model.</p>
<h3 id="memory">Memory</h3>
<p>Typical memory usage for primitive types: - Boolean (1); Char (2);
Double (8); Int (8) - Int[][] ~4MN; int[] ~4N+24;</p>
<p>Typical memory usage for objects in Java: - Object overhead 16 bytes
- Ref 8 bytes - Padding multiple of 8 bytes</p>
<h2 id="week-2-stacks-and-queues">Week 2: Stacks and Queues</h2>
<h3 id="stacks-and-queues">Stacks and queues</h3>
<ul>
<li>They’re fundamental data types</li>
<li>Stack has push and pop (LIFO)</li>
<li>Queue has enqueue and dequeue (FIFO)</li>
</ul>
<h3 id="stacks">Stacks</h3>
<ul>
<li>push(); pop(); isEmpty()</li>
<li>We build the stack with a LinkedList
<ul>
<li>push - insert node to the beginning</li>
<li>pop - remove node from the beginning</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb2"><pre
class="sourceCode java"><code class="sourceCode java"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pop</span><span class="op">()</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">String</span> item <span class="op">=</span> first<span class="op">.</span><span class="fu">item</span><span class="op">;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>first <span class="op">=</span> first<span class="op">.</span><span class="fu">next</span><span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">push</span><span class="op">()</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">Node</span> oldfirst <span class="op">=</span> first<span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>first <span class="op">=</span> <span class="kw">new</span> <span class="bu">Node</span><span class="op">();</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>first<span class="op">.</span><span class="fu">item</span> <span class="op">=</span> <span class="st">&quot;not&quot;</span><span class="op">;</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>first<span class="op">.</span><span class="fu">next</span><span class="op">=</span> oldfirst</span></code></pre></div>
<ul>
<li>Every op takes constant time in the worst case.</li>
<li>A tack with N items uses ~40N bytes.</li>
<li>Every object in Java has 16 bytes of overhead.</li>
</ul>
<p><strong>Alternative Implementation</strong> - Use array s[] to store
N items on stack - push(): add new item at S[N] - pop(): remove item
from S[N-1] - Cons: need to define the capacity ahead of time</p>
<p>We have to worry about loitering in Java. To avoid that we need to
set the removed item to Null so it can be reclaimed by the garbage
collector.</p>
<p><strong>Resizing arrays implementations</strong></p>
<p>Q: How can we grow the array? A: If the array is full, create a new
array of twice the size and copy the items. ~3N.</p>
<p>Q: How to shrink array? A: Wait until the array is one-quarter full.
Invariant; if the array is always between 25% and 100%</p>
<p>The worst case for push and pop will be N.</p>
<p><strong>Memory:</strong> It uses between 8N and 32N.</p>
<h3 id="tradeoffs">Tradeoffs</h3>
<ul>
<li>Linked list:
<ul>
<li>Every op takes constant time in worst case</li>
<li>We need to use extra time and space to deal with the links</li>
</ul></li>
<li>Stack
<ul>
<li>Every op takes constant amortized time</li>
<li>Less wasted space</li>
</ul></li>
</ul>
<h3 id="queues">Queues</h3>
<p><strong>LinkedList</strong></p>
<pre><code>enqueue()

String item = first.item;
first = first.next;
return item;

dequeue()

Node oldlast = last;
Node last = new Node();
last.item =&quot;not&quot;;
last.next = null;

oldlast.next = last;</code></pre>
<h3 id="generics">Generics</h3>
<p>We have implemented stacks of strings, but what about URLs, Ints,
Vans…</p>
<p>We use the generic type name in Java <code>&lt;Item&gt;</code></p>
<p>Java doesn’t allow generic array creation. We need to create an array
of Objects and pass it to a list of items.</p>
<pre><code>s = (Item[]) new Object([Capacity])</code></pre>
<p>Q: What to do about primitive types?</p>
<p>Wrapper type: each primitive type has a wrapper object type. Integer
is a wrapper type for int.</p>
<h3 id="iteration">Iteration</h3>
<p>Design challenge: How can we support iteration over stack items by
items.</p>
<p>An Iterable is a method that returns an Iterator. It has methods
hasNext() and next()</p>
<p>Why make data structures Iterable? It can helps us write elegant
code. Allows us to use <code>for each statements</code></p>
<h3 id="week-2-self-questions">Week 2: Self Questions</h3>
<ul>
<li>Why does the stack have contant push and pop time?
<ul>
<li>Because we just need to return the first item or last item in the
structure without iterating over its values</li>
</ul></li>
<li>What are advantages of a LinkedList over a stack?
<ul>
<li>Arrays provide immediate access to any item; but we need to know the
size on initialization</li>
<li>Linked List use space proportional to size but we need a reference
to access an item</li>
</ul></li>
<li>When do we use the LinkedList, when do we use the stack
<ul>
<li>Depending on the access patterns and memory consumption; stacks are
easy for push/pop but to find items it takes a bit more time. We do have
resizing issues.</li>
</ul></li>
<li>What’s the time complexity of each one?</li>
<li>How does Java deal with Generics and Iterators?</li>
<li>Why is it better to use Iterators? What does that allows us to
do?</li>
<li>What is the origin of LinkedLists?
<ul>
<li>They were initially used by LISP in 1950s as the primary structure
for all programs and data. IT presented challenges because they’re hard
to debug.</li>
</ul></li>
<li>Why can’t we remove an item a O(n) complexity from a Stack?
<ul>
<li>Because we still have to shift all the other positions next to the
item removed. This would cost a linear amount of time.</li>
</ul></li>
</ul>
<h2 id="week-2-sorting">Week 2: Sorting</h2>
<p><strong>Q:</strong> How does Sort knows how to sort Doubles, Strings,
and files without any information? - We use callbacks: the sort()
function calls the object’s compareTo method. - Commonly implemented in
other programming languages.</p>
<p>The sort implementation takes a Comparable[] object. Then call the
<code>.compareTo()</code> code.</p>
<p>Yes, you’re right. Let’s break down the definitions for each of these
properties in the context of a binary relation ( <span
class="math inline">≤</span> ) on a set ( S ):</p>
<ol type="1">
<li><strong>Antisymmetry</strong>:
<ul>
<li>This means that two distinct elements cannot both be “less than or
equal to” each other. If they are, they must actually be the same
element.</li>
</ul></li>
<li><strong>Transitivity</strong>:
<ul>
<li>This captures the intuitive idea that if ( a ) is “less than or
equal to” ( b ) and ( b ) is “less than or equal to” ( c ), then ( a )
should also be “less than or equal to” ( c ).</li>
</ul></li>
<li><strong>Totality (or Completeness)</strong>:
<ul>
<li>This means that given any two elements in the set, one must be “less
than or equal to” the other or vice-versa. This ensures that there are
no two elements which can’t be compared in the order.</li>
</ul></li>
</ol>
<p>Ex standard order for natural and real numbers, alphabetical orders,
chronological orders.</p>
<p><strong>Comparable API</strong> - Less than return -1 - Equal return
0 - More than return +1</p>
<h3 id="selection-sort">Selection Sort</h3>
<p>In iteration I, find index min of smallest remaining entry; swap a[I]
and a[min]</p>
<p>Algorithm: scans from left to right.</p>
<p>Selection sort uses <span
class="math inline"> <em>N</em><sup>2</sup>/2</span> compares. It takes
quadratic time, even if the input is sorted.</p>
<p><img
src="https://www.programiz.com/sites/tutorial2program/files/Selection-sort-0.png" /></p>
<h3 id="insertion-sort">Insertion Sort</h3>
<p>In iteration I we swap a[i] with each larger entry to its left.</p>
<p>To sort a randomly ordered arra with distinct keys, it uses <span
class="math inline"> 1/4<em>N</em><sup>2</sup></span> compares and <span
class="math inline"> 1/4<em>N</em><sup>2</sup></span> exchanges.</p>
<p>If the array is in reverse order, it has to go all the way back to
the beginning; if it’s in ascending order, it takes 0 exchanges.</p>
<p>For partially sorted arrays, it takes linear time. An array is
partially sorted if the number of inversions is less than some constant
cN.</p>
<figure>
<img
src="https://media.geeksforgeeks.org/wp-content/uploads/insertionsort.png"
alt="insertion sort" />
<figcaption aria-hidden="true">insertion sort</figcaption>
</figure>
<h3 id="shellsort">Shellsort</h3>
<p>Move entries more than one position at a time by h-sorting the array.
An h-sorted array is h interleaved sorted subsequences. (1959 sorting
method)</p>
<p>H-sort is just insertion sort with stride length h. Why do this? -
Big increments -&gt; small subarray - Small increments -&gt; nearly in
order</p>
<p>Which increment sequence to use? - Powers of two? Powers of two minus
one? - 3x+1? Easy to compute - Sedgewick: 1,5,19,41…</p>
<p>The worst case number of compares used by shell sort is <span
class="math inline"><em>O</em>(<em>N</em><sup>(3/2)</sup>)</span></p>
<p>Shell sort is a simple idea leading to substantial performance gains.
It involves very little code, used often in hardware (embedded
systems)</p>
<figure>
<img
src="https://static.javatpoint.com/ds/images/shell-sort-algorithm2.png"
alt="shell sort" />
<figcaption aria-hidden="true">shell sort</figcaption>
</figure>
<h3 id="shuffle-sort">Shuffle sort</h3>
<ul>
<li>Generate a random real number for each array entry</li>
<li>Sort using these random numbers as the key; uniformly random
permutation</li>
</ul>
<p>Knuth sort: in iteration I, pick integer r between 0 and I uniformly
at random. Then we swap a[I] and a[r].</p>
<h3 id="convex-hull">Convex Hull</h3>
<p>The convex hull of a set of N points is the smallest perimeter fence
enclosing the points.</p>
<p><img
src="https://miro.medium.com/v2/resize:fit:677/1*F4IUmOJbbLMJiTgHxpoc7Q.png" /></p>
<p>E.g., - find shortest path in the plane from s to t avoiding a
polygonal obstacle. - find the farthest path between two points</p>
<p>Graham scan demo - Choose point p with smallest y-coordinate - Sort
points by polar angle with p - Consider points in oder,, discard unless
it creates a ccw turn</p>
<p><strong>Week 3: Self-Questions</strong> - When should I use which
sort? - Insertion Sort - Select Sort - Merge Sort - Shell Sort -
Shuffling - Why are some sorts better than others in specific occasions?
- What are the most widely used sorts?</p>
<h2 id="week-3-merge-sort">Week 3: Merge Sort</h2>
<ul>
<li>Divide array into two halves</li>
<li>Recursively sort each array</li>
<li>Merge the two halves</li>
</ul>
<p>Copy to an auxiliary array. ** <img
src="https://www.programiz.com/sites/tutorial2program/files/merge-sort-example_0.png"
alt="mergesort" /></p>
<p>Mergesort uses at most NlgN compares and 6NlgN array accesses to sort
any array of size N.</p>
<p>Mergesort uses extra space proportions to N.</p>
<p>Practical improvements: - Use insertion sort for small subarrays;
merge sort has too much overhead for tiny arrays - The cut-off for
mergesort is 7 items - Stop if already sorted; is the biggest item in
first half &lt; smallest item in second half</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># split in half</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> n <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># recursive sorts</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>sort a[<span class="fl">1.</span>.m]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>sort a[m<span class="op">+</span><span class="fl">1.</span>.n]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># merge sorted sub-arrays using temp array</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> copy of a[<span class="fl">1.</span>.m]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span>, j <span class="op">=</span> m<span class="op">+</span><span class="dv">1</span>, k <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> i <span class="op">&lt;=</span> m <span class="kw">and</span> j <span class="op">&lt;=</span> n,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    a[k<span class="op">++</span>] <span class="op">=</span> (a[j] <span class="op">&lt;</span> b[i]) ? a[j<span class="op">++</span>] : b[i<span class="op">++</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    → invariant: a[<span class="fl">1.</span>.k] <span class="kw">in</span> final position</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> i <span class="op">&lt;=</span> m,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    a[k<span class="op">++</span>] <span class="op">=</span> b[i<span class="op">++</span>]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    → invariant: a[<span class="fl">1.</span>.k] <span class="kw">in</span> final position</span></code></pre></div>
<h3 id="bottom-up-merge-sort-no-recursion">Bottom-up Merge Sort (no
recursion)</h3>
<p>Basic plan - Pass through array merging subarrays of size 1 - Repeat
for subarrays of size 2,4,8,16… - Uses space proportional to the
array</p>
<h3 id="sorting-complexity">Sorting Complexity</h3>
<p>Computational complexity: framework to study efficiency of algorithms
to solve problem X.</p>
<ul>
<li>Model of computation: allowable ops.</li>
<li>Cost model: operation counts.</li>
<li>Upper bound: cost guarantee provided by some algorithm</li>
<li>Lower bound: Proven limit on cost for all algorithms of X</li>
<li>Optimal algorithm: best possible cost guarantee (lower bound - upper
bound)</li>
</ul>
<p><strong>E.g. Sorting</strong> - Model of computation: decision tree -
Cost model: # of compares - Upper bound: ~N log N - Lower bound: ~N log
N - Optimal algorithm: mergesort - Optimal with respect to # of compares
- Not optimal with respect to usage - We use theory as guide</p>
<h3 id="comparators">Comparators</h3>
<p>We also have a comparator interface to sort using an alternate
order.</p>
<p><code>int compare(Key v, Key w)</code></p>
<p>E.g., - Compare students by name, by section, etc…</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode java"><code class="sourceCode java"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">Arrays</span><span class="op">.</span><span class="fu">sort</span><span class="op">(</span>a<span class="op">,</span> Student<span class="op">.</span><span class="fu">BY_NAME</span><span class="op">)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">Arrays</span><span class="op">.</span><span class="fu">sort</span><span class="op">(</span>points<span class="op">,</span> p<span class="op">.</span><span class="fu">POLAR_ORDER</span><span class="op">)</span></span></code></pre></div>
<h3 id="stability">Stability</h3>
<p>A typical application: first sort by name, then by section.</p>
<p>A stable sort is one that presents the same order of items with a
given key.</p>
<p>Insertion sort and merge sort are stable (but not selection sort or
shell sort)</p>
<p>We can move items past items that are equal.</p>
<h3 id="self-questions">Self Questions</h3>
<ul>
<li>When do we care about stability for sorting</li>
<li>Why are some algorithms stable and others aren’t?</li>
</ul>
<h3 id="week-3-quick-sort">Week 3: Quick-sort</h3>
<p>Basic Plan: - Shuffle the array - Partition so that, for some k -
entry a[j] is in place - no larger entry to the left of j - no smaller
entry to the right of j - Sort each piece recursively</p>
<p>Summary: - We keep swapping 2 pointers as long as the left is less
than the right - Once they cross we stop and swap the lo with J</p>
<p>Implementation details: - Partitioning is in-place: using an extra
array can make partitioning easier - Terminating the loop: tricky to
check if pointers cross - Shuffling is needed for the performance
guarantee</p>
<p>Performance: - Number of compares 1.39 N Log N (39% more than merge
sort) - Worst case is quadratic but not likely</p>
<p>Properties: - Quick-sort is an in-place sorting algorithm - Quick
sort is not stable - We can improve it with median-of-3 and cutoff to
insertion sort</p>
<h3 id="week-3-selection-sort">Week 3: Selection Sort</h3>
<p><strong>Goal</strong>: Given an array of N items, find the Kth
largest. i.e., return the top K.</p>
<p>Quick Select: - Partition array so that: entry a[j] is in place - No
larger entry to the left of j - No smaller entry to the right of j -
Repeat in one subarray, depending on j; finished when j equals k.</p>
<p>3-Way Quick sort (Dijkstra Implementation) - We have 3 pointers; move
the ones less than the value to the left - Move the ones higher than the
value to the right - We increment the pointer if the value is equal to
the current value - We end up with all values even the duplicates
sorted</p>
<h3 id="week-3-applications">Week 3: Applications</h3>
<ul>
<li>Sort a list of names</li>
<li>Find the median</li>
<li>Data compression</li>
<li>Computer graphics</li>
<li>Load balancing on a parallel computer</li>
</ul>
<p><strong>Each sort type will have different attributes:</strong> -
Stable - Parallel - Deterministic - Keys all distinct - Multiple key
types - Linked lists or arrays - Large or small items - Is the array
randomly ordered - Do we need a guaranteed performance</p>
<p>System sorts cannot possibly cover all possible algorithms.</p>
<h3 id="week-3-self-questions">Week 3: Self Questions</h3>
<ul>
<li>Why does Arrays.sort() Arrays.sort() in Java use mergesort instead
of quicksort when sorting reference types?
<ul>
<li>The Java API for Arrays.sort() for reference types requires that it
is stable and guarantees nlogn performance. Neither of these are
properties of standard quicksort.</li>
</ul></li>
</ul>
<h2 id="week-4">Week 4</h2>
<h3 id="priority-queues-api">Priority Queues API</h3>
<p>Collections: - Stack - Queue - Randomized queue - Priority queue -
remove largest or smallest item</p>
<p>Applications: - Statistics - Artificial Intelligence - Number theory
- Graph searching - Operating systems - load balancing</p>
<p>Implementation: - It allows us to find the largest M items in a steam
of N items in NlogM (as compared with other data structures that have
NM)</p>
<p>The binary heap comes from a complete binary tree. Perfectly balanced
except for the bottom level.</p>
<p>Heap-ordered binary tree - Keys in nodes - Parent’s key no smaller
than children’s keys</p>
<figure>
<img
src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20221220165711/MinHeapAndMaxHeap1.png"
alt="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20221220165711/MinHeapAndMaxHeap1.png" />
<figcaption
aria-hidden="true">https://media.geeksforgeeks.org/wp-content/cdn-uploads/20221220165711/MinHeapAndMaxHeap1.png</figcaption>
</figure>
<p>If we insert a key larger than its parents we exchange them until the
order is restored.</p>
<p>To insert into a heap, we add a node at the end then swim it up with
at most 1 + lg N compares. AKA Peter principle; promoted to level of
incompetence.</p>
<p>If the parent’s key becomes smaller than one or both, we figure out
which children is higher then exchange them until the order is restored
(recursively). AKA better subordinate promoted (power struggle)</p>
<p>To delete the maximum, we exchange root with node at end, then sink
it down. At most 2 Log N compares.</p>
<p>Considerations: - Client does not change keys while they’re on the PQ
- Best practice: use immutable keys</p>
<p>Java Note on Immutability: when we use <em>final</em> those values
can’t change once created.</p>
<p>Immutable: String, Integer, Double, Color, Vector, Transaction…</p>
<p>Why use? - Simplifies debugging - Safer - Simplifies concurrent
programming - Safe to use as key</p>
<p><em>We should make classes immutable as much as possible - Creator of
Java</em></p>
<h3 id="week-3-heapsort">Week 3: Heapsort</h3>
<ul>
<li>We start with an unordered list of numbers; we transform it into a
max heap (“heapify”)</li>
<li>Then we replace the item at the root with the last item. Repeating
until the size of the heap is greater than 1</li>
</ul>
<p><strong>Complexity</strong> - Best Case: O(n log n) - Average Case:
O(n log n) - Worst Case: O(n log n)</p>
<p>Significance: In-place sorting algorithm with NlogN worst-case. -
Mergesort: no, linear extra space - Quicksort, no, quadratic time in
worst case - Heapsort: yes</p>
<h2 id="week-4-symbol-tables">Week 4: Symbol Tables</h2>
<ul>
<li>Insert a value with a specified key</li>
<li>Given a key, search for the corresponding value</li>
</ul>
<p><strong>DNS lookup</strong> - Insert URL with specified IP address -
Given URL, find corresponding IP address</p>
<p>API Examples - put - get - delete - contains - isEmpty - size -
keys</p>
<p>We associate one value with each key.</p>
<p>Conventions: - Values are not null - Method get() returns null if key
not present - Method put() overwrites old value with new value</p>
<p>Value type: any generic type. - Assume keys are Comparable: we use
compareTo() - Assume keys are any generic type; use equals() to est
equality</p>
<p>Best practices - use immutable types for symbol table keys. -
Immutables in Java: String ,Integer, Double, File… - Mutable in Java:
StringBuilder, java.net, URL, arrays</p>
<p>All Java classes inherit a method equals() - Reflexive - Symmetric -
Transitive - Non-null</p>
<h3 id="symbol-table-implementations">Symbol Table Implementations</h3>
<p><strong>Sequential Search (Unordered LinkedList)</strong> -
Performance - Search - N - Insert - N - Search Hit - N/2 - Inser - N -
Key Interface - equals()</p>
<p>We use a LinkedList in this implementation.</p>
<p><strong>Binary search in an ordered array</strong> - Data structure:
maintain an ordered array of key-value pairs - We use a rank helper
function to find how many keys &lt; K - Performance - Worst - Search -
log N - Insert N - Avg Case - Search Hit - log N - Inser - N/2 - Key
Interface: - compareTo()</p>
<h3 id="ordered-operations">Ordered Operations</h3>
<ul>
<li>We want to be able to do min(); get(); floor(); select(7); ceiling()
etc… on a symbol table</li>
<li>These ops come natural in a binary search operation</li>
</ul>
<p>We can add more operations to a Symbol Table such as: - deleteMin() -
deleteMax() - select() - rank() - min() / max() - ordered iteration</p>
<p>We normally argue against wide interfaces.</p>
<h3 id="binary-search-trees">Binary Search Trees</h3>
<p>A BST is a binary tree in symmetric order.</p>
<p>It is either: - Empty - Two disjoint binary trees (left and
right)</p>
<p>Each node has a key and every node’s key is: - Larger than all the
keys in its left subtree - Smaller than all the keys in its right
subtree</p>
<pre><code>
private class Node
    {
    private Key key;
    private Value val;
    private Node left, right;
    
    public Node(Key key, Value val)
    {
        this.key = key;
        this.val = val;
    }
    }

public Value get(Key key)
{
    Node x = root;
    while (x != null)
    {
        int cmp = key.compareTo(x.key)
        if (cmp &lt; 0) x = x.left;
        else if (cmp &gt; 0) x = x.right;
        else if (cmp == 0) return x.val;
    }
    return null;
}</code></pre>
<h2 id="week-5-balanced-search-trees">Week 5: Balanced Search Trees</h2>
<h3 id="search-trees">2-3 Search Trees</h3>
<p>Allow 1 or 2 keys per node. 2 node equals one key with two children.
3 node equals 2 keys with 3 children.</p>
<p>Every path from root to null has same length. Inorder traversal
yields keys in ascending order.</p>
<p>Search: - Compare search key against keys in node - Find interval
containing search key - Follow associated link (rec)</p>
<p>Inserting - We want to keep the 2 key 3 node or 1 key 2 node
strategy. Therefore we need to insert and check recursively so that the
condition is met across the nodes we changed. - We may have some
temporary 4 nodes; we split them and convert them to 1 key 2 nodes and
increase the height by 1 - Splitting a 4 node is a local transformation
with a constant number of operations</p>
<p>Tree Height - Worst case: log N (all 2 nodes) - Best case: log3N or
.631 log N (all 3 nodes)</p>
<h3 id="red-black-b-trees">Red Black B-Trees</h3>
<p>A red-black tree is a binary search tree with one extra bit of
storage per node: its color, which can be either RED or BLACK. By
constraining the node colors on any simple path from the root to a leaf,
red-black trees ensure that no such path is more than twice as long as
any other, so that the tree is approximately balanced.</p>
<p>A red-black tree is a binary tree that satisfies the following
red-black properties: 1. Every node is either red or black. 2. The root
is black. 3. Every leaf (NIL) is black. 4. If a node is red, then both
its children are black. 5. For each node, all simple paths from the node
to descendant leaves contain the same number of black nodes.</p>
<p>Time Complexity: - Search - O(Log N) - Insert - O(Log N) - Remove -
O(Log N)</p>
<figure>
<img
src="https://media.cheggcdn.com/media%2F47a%2F47a9c017-f857-4c93-be2c-0a8f23194b06%2FphpDyOAKI.png"
alt="https://media.cheggcdn.com/media%2F47a%2F47a9c017-f857-4c93-be2c-0a8f23194b06%2FphpDyOAKI.png" />
<figcaption
aria-hidden="true">https://media.cheggcdn.com/media%2F47a%2F47a9c017-f857-4c93-be2c-0a8f23194b06%2FphpDyOAKI.png</figcaption>
</figure>
<p>Properties: - No node has two red links connected to it - Every path
from root to null link has same number of black links - Red links lean
left</p>
<p><strong>Search</strong></p>
<p>Search is the same after as for elementary BST (ignore color). Most
other operations are also identical</p>
<p>We now add a boolean color to the class.</p>
<pre><code>private class Node {
    Key key;
    Value val;
    Node left, right;
    boolean color;
}</code></pre>
<p><strong>Rotations</strong></p>
<p>Left rotation: - Check the node is red - Orient a temporarily right
leaning red link to lean left</p>
<p>Right rotation: - Orient a left leaning red link to temporarily lean
right</p>
<p>Color flip: - Recolor to split a temporary 4-node</p>
<p>With the operations above, we maintain 1-1 correspondence with 2-3
trees by applying elementary red-black BST operations</p>
<p>Balance: - The height of the tree is less than 2 Log N in the worst
case. - Height of the tree is 1.00logN in typical applications</p>
<h2 id="geometric-applications-of-bsts">Geometric Applications of
BSTs</h2>
<p>Intersection among geometric objects. E.g., 2d orthogonal range
search.</p>
<p>Applications in CAD, games, movies, virtual reality, databases…</p>
<p>Extending ordered symbol table: - Range search: adding range search
btw k1 and k2 - Range count: how many keys between k1 and k2</p>
<p>Geometric Interpretation: - Keys are point on a line - Find/count
points in a given 1d interval</p>
<h3 id="line-segment-intersection">Line Segment Intersection</h3>
<p>Given N horizontal and vertical line segments, find all
intersections.</p>
<p>Sweep-line algorithm: - x coordinates define events - h-segment (left
endpoint): insert your coordinates into the BST</p>
<p>It takes time proportional to NlogN+R to find all R
intersections.</p>
<h3 id="kd-trees">Kd-Trees</h3>
<p>We now have 2 dimensional keys. We want to: - insert a 2d key -
search for a 2d key - Range search - find all keys that lie in a 2d
range - Range count - count all keys in a 2d range</p>
<p>Keys are point in the plane. We can find and count the points in a
given h-v rectangle.</p>
<p><strong>Range Search</strong></p>
<p>Space Time Tradeoff: - Space: M^2 + N - Time: 1 + N/M^2</p>
<p>Running time: - Init N - Inser 1 - Range search 1 per point range</p>
<p>Problem: Points are not evenly distributed - e.g., USA Map</p>
<figure>
<img
src="https://mma.prnewswire.com/media/1311215/US_Tree_Map_1200x757.jpg?p=twitter"
alt="usa map" />
<figcaption aria-hidden="true">usa map</figcaption>
</figure>
<p>We space partition the trees to represent a recursive subdivision of
2d space - Grid: divide space into squares - 2d tree: recursively divide
into half-lines</p>
<p>Applications: - Accelerate render in Doom - Flight simulators -
Nearest neighbor search - Astronomical databases</p>
<figure>
<img src="https://i.stack.imgur.com/Hs9p6.png"
alt="https://i.stack.imgur.com/Hs9p6.png" />
<figcaption
aria-hidden="true">https://i.stack.imgur.com/Hs9p6.png</figcaption>
</figure>
<p>We partition according to the points and their coordinates. We
alternate which coordinates we use as the key. E.g., left child go down,
right child go up.</p>
<p>Range search: - Find all point in a query axis aligned rectangle -
Check if point in node lies in given rectangle - Rec search left/bottom
- Rec search right/top - Typical case: R+logN - Worst case scenario:
R+sqrt(N)</p>
<p><strong>Nearest Neighbor Search</strong> - Typical case: log N -
Worst case: N - Algorithm - Check dist from point in node to query - Rec
search left/bottom - Rec search right/top - Organize so we begin by
searching for query point</p>
<figure>
<img src="https://i.stack.imgur.com/hhZE1.jpg"
alt="https://i.stack.imgur.com/hhZE1.jpg" />
<figcaption
aria-hidden="true">https://i.stack.imgur.com/hhZE1.jpg</figcaption>
</figure>
<h3 id="flocking-boids">Flocking Boids</h3>
<p>3 simple rules lead to complex emergent behavior: - collision
avoidance: point away from k nearest boids - flock centering: point
towards the center of mass of k nearest boids - velocity matching:
update velocity to the avg of k nearest boids</p>
<figure>
<img
src="https://repository-images.githubusercontent.com/258305543/28971980-92d2-11ea-8a66-4d0d91c0e790"
alt="https://repository-images.githubusercontent.com/258305543/28971980-92d2-11ea-8a66-4d0d91c0e790" />
<figcaption
aria-hidden="true">https://repository-images.githubusercontent.com/258305543/28971980-92d2-11ea-8a66-4d0d91c0e790</figcaption>
</figure>
<p><strong>KD trees are a simple data structure for processing
k-dimensional data:</strong> - widely used - adapts well to high
dimensional and clustered data - discovered by an undergrad in an
algorithms class</p>
<h3 id="d-interval-search">1D Interval Search</h3>
<p>Data structure to hold a set of overlapping intervals.</p>
<p>We want to: - Insert - Search - Delete - Check for an
intersection</p>
<p>E.g., which intervals intersect (9,16)?</p>
<p>Create a BST, each node contains an interval (lo, hi) - We use the
left endpoint as BST key - Store max endpoint in subtree rooted at
node</p>
<p>Search: - If interval node intersects, return it - Else if left
subtree is null, go right - Else if max endpoint in left subtree is less
than lo, go right - Else go left</p>
<figure>
<img
src="https://media.geeksforgeeks.org/wp-content/uploads/20200708073504/interval.jpg"
alt="https://media.geeksforgeeks.org/wp-content/uploads/20200708073504/interval.jpg" />
<figcaption
aria-hidden="true">https://media.geeksforgeeks.org/wp-content/uploads/20200708073504/interval.jpg</figcaption>
</figure>
<p>We can use a Red Black BST to guarantee performance of the
search.</p>
<p>Most operations have a brute of 1/N; for interval search trees we
have Log N for most cases.</p>
<h3 id="rectangle-intersection-problem">Rectangle Intersection
Problem</h3>
<p>Problem: Find all intersections among a set of N orthogonal
rectangles.</p>
<p>Important problem when designing microprocessors. Designing a
computer became a geometric problem.</p>
<p>They had specific design rules: - Some wires cannot intersect -
Certain spacing needed between diff types of wires - Debugging:
orthogonal intersection search</p>
<p>We sweep a vertical line from left to right. IT takes proportional to
N log N + R log N to find R intersections among a set of N
rectangles</p>
<p>It reduces the rectangle problem to a 1 dimensional problem.</p>
<figure>
<img
src="https://miro.medium.com/v2/resize:fit:1400/1*4BRsVG6P-Tw-ApBgNlAyDA.png"
alt="https://miro.medium.com/v2/resize:fit:1400/1*4BRsVG6P-Tw-ApBgNlAyDA.png" />
<figcaption
aria-hidden="true">https://miro.medium.com/v2/resize:fit:1400/1*4BRsVG6P-Tw-ApBgNlAyDA.png</figcaption>
</figure>
<h2 id="week-6-hashing">Week 6: Hashing</h2>
<h3 id="hash-tables">Hash Tables</h3>
<p>Use a hash function that takes the key and reduces it to an array
index.</p>
<p>It can cause some issues: - Computing the hash function - Equality
test: need check whether two keys are equal - Collision resolution: so
many values; we may have 2 values for the same array index</p>
<p>Hashing is a classic space-time tradeoff - No space limitation -
trivial hash function with key as index - No time limitation - trivial
collusion resolution with linear probing</p>
<p>Ideal goal: - Scramble the keys uniformly to produce a table index -
Efficiently computable - Each table index equally likely for each
key</p>
<p>Hashing is widely used in system programming. Hash codes return a 32
bit value.</p>
<p>There are hash codes for each data type: integers, booleans, doubles,
and strings.</p>
<p>A recipe for user defined types is: - Combine each significant field
using the 31x + y rule - If the field is a primitive type, use wrapper
type: hashCode() - If field is null, return 0 - If field is a reference
type, use hashCode()</p>
<p>The hash code can be a value between <span
class="math inline"> − 2<sup>31</sup></span> and <span
class="math inline">2<sup>31</sup></span>. The hash function has an int
between 0 and M-1.</p>
<p>Our <strong>main assumption</strong> is that each key is equally
likely to hash an int between 0 and M-1.</p>
<h2 id="week-7-graphs">Week 7: Graphs</h2>
<p><strong>Graph</strong>: A set of vertices connected pairwise by
edges.</p>
<p>Why graphs? Many applications, broadly useful, challenging branch of
computer science and discrete math.</p>
<figure>
<img
src="https://cambridge-intelligence.com/wp-content/uploads/2015/07/enron-network-visualization-6.png"
alt="Enron Emails" />
<figcaption aria-hidden="true">Enron Emails</figcaption>
</figure>
<ul>
<li>Path: Sequence of vertices connected by edges.</li>
<li>Cycle: path whose first and last vertices are the same</li>
</ul>
<p>Two vertices are connected if there’s a path between them.</p>
<p>Some common challenges: - Is there a path between A and B? - What is
the shortest path? - Is there a cycle in the graph? - Is there a cycle
that uses each edge exactly once? - How can we connect all of the
vertices? - What is the best way to connect all the vertices?</p>
<p><strong>Graph API</strong></p>
<pre><code>Graph(int V)
Graph(In in)
void addEdge(int v, int w)
Iterable&lt;Int&gt; adj(int v) - vertices adjacent to v
int V() - num of vertices
int E() - nuim of edges
String - toString()</code></pre>
<p>The most widely used representation of graphs is the adjacency-list
graph representation.</p>
<figure>
<img
src="https://media.geeksforgeeks.org/wp-content/uploads/20230727154843/Graph-Representation-of-Undirected-graph-to-Adjacency-List.png"
alt="Adjacency List Graph" />
<figcaption aria-hidden="true">Adjacency List Graph</figcaption>
</figure>
<p>We use a data structure of type:
<code>Bag&lt;Integer&gt; [] adj;</code>. To create the graph we use
<code>adj[v]</code>.</p>
<p>To add edges we use <code>adj[v].add(w)</code> and
<code>adjust[w].add(v)</code></p>
<h3 id="depth-first-search">Depth First Search</h3>
<p>It can be visualized with a maze. We explore every intersection in
the maze.</p>
<p>Algorithm:</p>
<ul>
<li>Unroll a ball of string behind you</li>
<li>Mark each visited intersection and passage</li>
<li>Retrace steps when no unvisited options</li>
</ul>
<pre><code>DFS (to visit a vertex v)
    Mark v as visited
    Recursively visit all unmarked vertices w adjacent to v</code></pre>
<p>Typical application: - Find all vertices connected to a given source
vertex</p>
<p>Design pattern for graph processing - Use a graph object - Decouple
the graph from the graph processing - Query the graph processing routine
for information</p>
<pre><code>Paths(Graph G, int s)
    boolean hasPathTo(int v)
    Itrerable&lt;int&gt; pathTO (int v)</code></pre>
<p>Data structure: - boolean[] marked - int[] edgeTo to keep track of
paths</p>
